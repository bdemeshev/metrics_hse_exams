% !TEX root = ../metrics_hse_exams.tex

\subsection{ИП, вспомнить всё!}

\begin{enumerate}
\item Найдите длины векторов $a=(1,1,1)$ и $b=(1,4,6)$ и косинус угла между ними. Найдите длину проекции вектора $b$ на вектор $a$.

\item Сформулируйте теорему Фалеса. Сформулируйте и докажите теорему Пифагора.

%\item На плоскости $\alpha$ лежит прямая $\ell$. Вне плоскости $\alpha$ лежит точка $C$. Ромео проецирует точку $C$ на прямую $\ell$ и получает точку $R$. Джульетта проецирует точку $C$ сначала на плоскость $\alpha$, а затем проецирует полученную точку $A$ на прямую $\ell$. После двух действий Джульетта получает точку $D$. Обязательно ли $R$ и $D$ совпадают?

\item Для матрицы

\[
A=\begin{pmatrix}
6 & 5 \\
5 & 6 \\
\end{pmatrix}
\]

\begin{enumerate}
\item Найдите собственные числа и собственные векторы матрицы
\item Найдите $\det (A)$, $\tr(A)$
\item Найдите собственные числа матрицы $A^{2017}$, $\det (A^{2017})$ и $\tr(A^{2017})$
\end{enumerate}

%\item Известно, что $X$ — матрица размера $n \times k$ и $n>k$, известно, что $X'X$ обратима. Рассмотрим матрицу $H=X(X'X)^{-1}X'$. Укажите размер матрицы $H$, найдите $H^{2016}$, $\tr(H)$, $\det(H)$, собственные числа матрицы $H$. Штрих означает транспонирование.

\item Занудная халява: известно, что $\Cov(X, Y)=5$, $\Var(X)=16$, $\Var(Y)=25$, $\E(X)=10$, $\E(Y)=-5$.
Найдите $\Cov(X+2Y, Y-X)$, $\Var(X+2Y)$, $\E(X+2Y)$.

\item Блондинка Маша 100 раз выходила на улицу и при этом 40 раз встретила динозавра. Постройте 95\% доверительный интервал для вероятности встретить динозавра. На уровне 5\% проверьте гипотезу о том, что данная вероятность равна $0.5$ против альтернативной гипотезы об отличии данной вероятности от $0.5$.

\item В кошельке 5 монеток, три золотых и две серебряных. Маша берёт наугад две монетки по очереди. Маше достались одинаковые монетки. Какова условная вероятность того, что обе золотые?
\end{enumerate}


\subsection{ИП, вспомнить всё!, ответы}

\begin{enumerate}
\item $|a| = \sqrt{3}$, $|b| = \sqrt{53}$, $\cos(a,b) = 11/\sqrt{159}$,
длина проекции равна $11/\sqrt{3}$.

\item Теорема Фалеса. Если параллельные прямые, пересекающие стороны угла,
отсекают на одной его стороне равные отрезки,
то они отсекают равные отрезки и на другой его стороне.

Теорема Пифагора. В прямоугольном треугольнике квадрат длины гипотенузы
равен сумме квадратов длин катетов.

\item
\begin{enumerate}
\item $\lambda_1 = 11$, $\lambda_2 = 1$, $v_1 = (1 \quad 1)'$, $v_2 = (-1 \quad 1)'$
\item $\det(A) = 11$, $\tr(A) = 12$
\item $\lambda_1 = 11^{2017}$, $\lambda_2 = 1$, $\det(A^{2017}) = 11^{2017}$, $\tr(A^{2017}) = 1 + 11^{2017}$
\end{enumerate}

\item $\Cov(X+2Y, Y-X) = 29$, $\Var(X+2Y) = 136$, $\E(X+2Y) = 0$

\item $\left[0.4 - 2 \sqrt{\frac{0.4\cdot0.6}{100}}; 0.4 + 2 \sqrt{\frac{0.4\cdot0.6}{100}} \right]$,
значение $0.5$ не входит в доверительный интервал, значит, основная гипотеза отвергается.

\item $3/4$
\end{enumerate}




\subsection{Контрольная 1, 26.10.2017}



\input{tests/2017_kr_01}

\AMCnumero{1} % начинаем нумерацию вопросов с 1го

\cleargroup{all}
\copygroup[10]{2017_kr_01}{all}
\insertgroup{all}


\subsubsection*{Часть 2. Задачи.}

\begin{enumerate}

\item Докажите, что для модели парной регрессии  $Y_i = \beta_1 + \beta_2 X_i  + \e_i$, оцененной с помощью МНК, сумма остатков регрессии  $e_i=Y_i - \hat{Y}_i$ равна 0.

\item Покажите, что для регрессий $\hat{Y}_i= \hb_1 + \hb_2 X_i$ , $\hat{X}_i = \hat{\alpha}_1 + \hat{\alpha}_2 Y_i$, оценённых по одной и той же выборке $(X_1, Y_1),\ldots, (X_n, Y_n)$ коэффициенты детерминации $R^2$ совпадают, а оценки коэффициентов наклона связаны соотношением $\hb_2 \hat{\alpha}_2 = R^2$.

\item Для классической регрессионной модели $Y_i = \beta_1 + \beta_2 X_i + \e_i , i = 1,\ldots, 20$ известно, что $\sum_{i=1}^{20} X_i = 12, \sum_{i=1}^{20} X_i^2 = 12, \sum_{i=1}^{20} Y_i = 60, \sum_{i=1}^{20} Y_i^2 = 220, \sum_{i=1}^{20} X_i Y_i = 48$. Найдите
\begin{enumerate}
\item $\hb_1, \hb_2$,
\item $TSS$,
\item $ESS$,
\item $\hat\sigma^2_{\varepsilon}$
\end{enumerate}

\item Для предыдущей задачи постройте точечный и 95\% интервальный индивидуальный прогноз в точке $X = 2$.

\item Заполните клетки с точками в приведенной ниже таблице. Клетки с XXX заполнять не надо.

\begin{tabular}{lr} \toprule
Показатель & Значение \\
\midrule
Multiple R          & XXX \\
$R^2$     			& \ldots \\
Standart error 		& XXX \\
Observations		& 60 \\
\bottomrule
\end{tabular}

ANOVA:

\begin{tabular}{lrrrrr} \toprule
      	   	 &  df 	& SS		& MS 	& F & Significance F \\
\midrule
Regression   & 1 	& 0.1  	& 	 	&  	& 		\\
Residual     & 59  	& 0.4  	&     	&  	&     	\\
Total        & 60  	& \ldots  	&    	&  	&     	\\
\bottomrule
\end{tabular}


\begin{tabular}{rrrrrr}
  \hline
 			& Coef. 	& St. error	& t-stat 	& Lower 95\% 	& Upper 95\% \\
  \hline
Intercept 	& 0.0045 	& 0.015 	& XXX 		& XXX 			& XXX \\
MARKET		& 0.56 		& 0.14 		& \ldots 	& \ldots 		& \ldots \\
   \hline
\end{tabular}

\end{enumerate}




\subsection{Контрольная 1, 26.10.2017, решения}

\begin{enumerate}
\item Воспользуемся свойством о том, что в парной рергессии точка $(\bar X, \bar Y)$
лежит на линии выборочной регрессии. Тогда
\begin{align*}
\sum_i e_i &= \sum_i (Y_i - \hat \beta_1 - \hat \beta_2 X_i) \\
\frac{1}{n} \sum_i e_i &= \bar Y_i -  \hat \beta_1 - \hat \beta_2 \bar X = 0 \\
\sum_i e_i &= 0
\end{align*}

\item МНК-оценки в регрессиях будут равны:
\begin{align*}
\hb_2 = \frac{\sum_{i=1}^n (X_i - \overline X)(Y_i - \overline Y)}{\sum_{i=1}^n (X_i - \overline X)^2} \\
\hat\alpha_2 = \frac{\sum_{i=1}^n (Y_i - \overline Y)(X_i - \overline X)}{\sum_{i=1}^n (Y_i - \overline Y)^2}
\end{align*}
Тогда их произведение равно
\begin{multline*}
\hb_2 \hat\alpha_2 = \frac{\sum_{i=1}^n (X_i - \overline X)(Y_i - \overline Y) \cdot \sum_{i=1}^n (Y_i - \overline Y)(X_i - \overline X)}{\sum_{i=1}^n (X_i - \overline X)^2 \cdot \sum_{i=1}^n (Y_i - \overline Y)^2} \\
= \frac{\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline X)(Y_i - \overline Y) \cdot \frac{1}{n-1}  \sum_{i=1}^n (Y_i - \overline Y)(X_i - \overline X)}{\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline X)^2 \cdot \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline Y)^2} \\
= \frac{\sCov^2(X,Y)}{\sVar(X)\sVar(Y)} = \sCorr^2(X,Y) = R^2
\end{multline*}
Поскольку $\sCorr^2(X,Y) = \sCorr^2(Y,X)$, коэффициенты детерминации в обеих регрессиях равны.

\item
\begin{enumerate}
\item По формулам МНК-оценок получаем:
\begin{align*}
&\hb_2 = \frac{\sum_{i=1}^{20} X_i Y_i - \bar X \sum_{i=1}^{20} Y_i}{\sum_{i=1}^{20} (X_i - \bar X)^2} = 2.5 \\
&\hb_1 = \bar Y - \hb_2 \bar X = 1.5
\end{align*}
\item $TSS = \sum_{i=1}^{20} (Y_i - \bar Y)^2 = 40$
\item $ESS = \sum_{i=1}^{20} (\hat Y_i - \bar Y)^2 = 30$
\item $\hat\sigma^2_{\varepsilon} = \frac{RSS}{n-k} = \frac{5}{9}$
\end{enumerate}

\item Точечный прогноз:
\[
\hat Y_f = 1.5 + 2.5 \cdot 2 = 6.5
\]
Для индивидуального прогноза понадобится:
\begin{align*}
\widehat{\Var}(Y_f - \hat Y_f) &= \widehat{\Var}(\beta_1 + \beta_2 X_f + \varepsilon_f - \hat Y_f) = \widehat{\Var}(\varepsilon_f - \hat Y_f) = \widehat{\Var}(\varepsilon_f) + \widehat{\Var}(\hat Y_f) \\
&= \hat\sigma^2_{\varepsilon_f} + \widehat{\Var}(\hb_1 + \hb_2 \cdot 2) = \hat\sigma^2_{\varepsilon_f} + \widehat{\Var}(\hb_1) + 4 \widehat{\Var}(\hb_2) + 4 \widehat{\Cov}(\hat \beta_1, \hat \beta_2) \\
&= \hat\sigma^2_{\varepsilon_f} + \frac{\hat\sigma^2_{\varepsilon_f}\sum_{i=1}^{20} X_i^2}{20\sum_{i=1}^{20}(X_i - \overline X)^2} + 4 \cdot \frac{\hat\sigma^2_{\varepsilon_f}}{\sum_{i=1}^{20}(X_i - \overline X)^2} - 4 \cdot \frac{\overline X \hat\sigma^2_{\varepsilon_f}}{\sum_{i=1}^{20}(X_i - \overline X)^2} \\
&\approx 0.81
\end{align*}
Из таблицы для $t_{20-2}$ получаем  $t = 2.1$. И интервальный прогноз примет вид:
\[
[6.5 - 2.1\sqrt{0.81}; 6.5 + 2.1\sqrt{0.81}]
\]
\[
[4.61; 8.39]
\]
\item $R^2 = ESS / TSS = 0.2$

$TSS = RSS + ESS = 0.5$

$t = (0.56 - 0) / 0.14 = 4$

Доверительный интервал: $[0.56 - 2\cdot 0.14; 0.56 + 2\cdot 0.14] = [0.28; 0.84]$, где $t_{60-2;0.975} = 2$.
\end{enumerate}




\subsection{Контрольная 2, 26.10.2017}



\subsubsection*{Часть 1. Тест.}


\input{tests/2017_kr_02_midterm}

\AMCnumero{1} % начинаем нумерацию вопросов с 1го

\cleargroup{all}
\copygroup[10]{2017_kr_02_midterm}{all}
\insertgroup{all}


\subsubsection*{Часть 2. Задачи.}



\begin{enumerate}

\item По 29 наблюдениям оценили функцию спроса на яблоки
\[
\widehat\ln Q = 14 -6 \ln P_{apple} + 3 \ln P_{orange} + 2 \ln P_{banana},
\]
где $Q$ — спрос на яблоки, а $P_{apple}$ — цена яблок, $P_{orange}$ — цена апельсинов и $P_{banana}$ — цена бананов.

Известна оценка ковариационной матрицы коэффициентов регрессии:
\[
\hVar(\hb) =
\begin{pmatrix}
1 & 0.1 & -0.2 & 0.3 \\
0.1 & 2 & 0.5 & 0.7 \\
-0.2 & 0.5 & 3 & 0.6 \\
0.3 & 0.7 & 0.6 &  4
\end{pmatrix}
\]

На уровне значимости 5\% проверьте гипотезу о том, что
$\beta_{orange}=\beta_{banana}$.


\item По ежегодным данным за 25 лет была оценена зависимость расходов на жилье $Y$ от доходов индивидуумов $I$ и относительного индекса цен $P$ с помощью трёх моделей:

$\hY = \underset{(46.9)}{-39.9} + \underset{(0.009)}{0.179} P + \underset{(0.409)}{0.113} I, R^2 = 0.987$

$\hY = \underset{(3.34)}{-27.03} + \underset{(0.004)}{0.177} I, R^2 = 0.987$

$\hY = \underset{(24.2)}{813.3} + \underset{(0.019)}{-7.08} P, R^2 = 0.76$

Известно, что $\hCorr (P, I) = -0.88$, в скобках указаны стандартные отклонения коэффициентов.

Какую модель Вы предпочтёте и почему?


\item Для регрессии в отклонениях $y = \beta_1 x + \beta_2 z + \varepsilon$, оцененной по 100 наблюдениям, известны следующие суммы:
\[
\sum y^2_i = \frac{493}{3}, \sum x_i^2 = 30, \sum z_i^2 = 3, \sum x_i y_i = 30, \sum z_i y_i = 20, \sum x_i z_i = 0
\]

Найдите оценки МНК коэффициентов $\beta_1, \beta_2$ и коэффициент детерминации $R^2$.


\item Приведены результаты оценки зависимости логарифма арендной платы жилья в России, lnPRICE, от общей площади,  GENSQUARE (кв. м.), наличия газа, GAS, (1 – есть, 0 – нет), наличия телефона, PHONE (1 – есть, 0 – нет).

Модели 1 и 2 оценены для городов с численностью населения более миллиона человек, модель 3 — для городов с численностью от полумиллиона до миллиона, модель 4 — для обеих выборок. В скобках в таблице приведены стандартные ошибки.

\begin{enumerate}
\item Проверьте значимость коэффициентов в модели 2 при уровне значимости 5\% и дайте интерпретацию полученным результатам.
\item Проверьте гипотезу о равенстве $0.02$ коэффициента при переменной GAS при уровне значимости $0.05$ в модели 2.
\item Можно ли утверждать, что наличие газа и телефона в крупном городе не влияет на стоимость аренды? Ответ обоснуйте  формулировкой и проверкой подходящей гипотезы.
\item Опираясь на результаты оценки моделей 2, 3 и 4, можно ли утверждать, что зависимость стоимости жилья от рассмотренных выше факторов едина для с городов с численностью населения более миллиона человек и с численностью от полумиллиона до миллиона? Ответ обоснуйте подходящим тестом.
\end{enumerate}

\begin{tabular}{l c c c c }
\toprule
 & Модель 1 & Модель 2 & Модель 3 & Модель 4 \\
\midrule
(Intercept) & $7.168$      & $6.884$    & $6.669$      & $6.6609$    \\
            & $(0.1334)$   & $(0.2398)$ & $(0.0619)$   & $(0.0619)$ \\
GENSQUARE   &  $0.012$     & $0.012$    & $0.0035$     & $0.00289$   \\
            & $(0.002687)$ & $(0.0027)$ & $(0.000902)$ & $(0.00089)$ \\
GAS         &              & $0.119$    & $-0.315$      & $-0.205$    \\
            &              & $(0.166)$  & $(0.049)$    & $(0.0478)$  \\
PHONE       &              & $0.197$    & $0.444$      & $0.547$    \\
            &              & $(0.123)$  & $(0.048)$   & $(0.0463)$  \\
\midrule
$R^2$        & 0.1330         & 0.1541      & 0.1186         & 0.1236     \\
F            & 19.95          & 7.78        & 46.85          & 55.35     \\
Adj. $R^2$   & 0.1264         & 0.1343      & 0.116         & 0.1214    \\
Num. obs.    & 132            & 132         & 1049           & 1181       \\
RSS          & 23.05          & 22.494      & 616.709       & 710.21    \\
$\hat\sigma$ & 0.42113        & 0.41921     & 0.7682        & 0.776    \\
\bottomrule
\end{tabular}


\end{enumerate}

\subsection{Контрольная 2, 26.10.2017, решения}

\begin{enumerate}
\item Для проверки гипотезы используем $t$-статистику:
\[
t_{obs} = \frac{\hb_{orange} - \hb_{banana} - (\beta_{orange} - \beta_{banana})}{se(\hb_{orange} - \hb_{banana})} \sim t_{n-k} = t_{29 -4} = t_{25}
\]
Сначала найдём стандартное отклонение:
\begin{align*}
se(\hb_{orange} - \hb_{banana}) &= \sqrt{\widehat{\Var}(\hb_{orange}) + \widehat{\Var}(\hb_{banana}) - 2 \widehat{\Cov}(\hb_{orange}, \hb_{banana})} \\
&= \sqrt{3 + 4 - 2\cdot 0.6} \approx 2.41
\end{align*}
Тогда наблюдаемое значение статистики равно $t_{obs} = \frac{3-2}{2.41} \approx 0.42$,
а $t_{crit} \approx 2.06$. Значит, оснований отвергать нулевую гипотезу нет.
\item В модели (1) $\widehat{\Corr}(P,I)=-0.8$ свидетельствует о проблеме мультиколлинеарности в данных.
В модели (3) значение $R^2$ самое низкое. Предпчтительной является модель $(2)$.
\item
\begin{align*}
\hat \beta_1 &= \frac{\sCov(X,Y)}{\sVar(X)} = \frac{30}{30} = 1 \\
\hat \beta_2 &= \frac{\sCov(Z,Y)}{\sVar(Z)} = \frac{20}{3}
\end{align*}
Для нахождения коэффициента детерминации воспользуемся факторным разложением:
\[
R^2 = \hb_1 \frac{\sCov(X,Y)}{\sVar(Y)} + \hb_2 \frac{\sCov(Z,Y)}{\sVar(Y)} = 1 \cdot \frac{30}{\frac{493}{3}} + \frac{20}{3} \cdot \frac{20}{\frac{493}{3}} = \frac{490}{493}
\]
\item
\begin{enumerate}
\item $n=132$, $k=3 \Rightarrow n-k = 128$, в таблице находим,
что при $\alpha = 0.05$ $t_{crit} = 1.98$.

$t_{obs} = \frac{6.884 - 0}{0.2398} = 28.7 > t_{crit} \Rightarrow \hat \beta_{Intercept}$ значим.

$t_{obs} = \frac{0.012 - 0}{0.0027} = 4.(4) > t_{crit} \Rightarrow \hat \beta_{GENSQUARE}$ значим.

$t_{obs} = \frac{0.119 - 0}{0.166} = 0.72 < t_{crit} \Rightarrow \hat \beta_{GAS}$ незначим.

$t_{obs} = \frac{0.197 - 0}{0.123} = 1.6 < t_{crit} \Rightarrow \hat \beta_{PHONE}$ незначим.

При прочих равных общая площадь оказывает влияние на логарифм арендной платы,
а наличие газа и наличие телефона — нет.
\item При верной $H_0$ и $\alpha = 0.1$ $t_{crit} = 1.658$.
\[
t_{obs} = \frac{0.199 - 0.02}{0.166} = -0.596
\]
Поскольку $|t_{obs}| < t_{crit}$, оснований отвергать нулевую гипотезу нет.
\item
\[H_0:
\begin{cases}
  \beta_{GAS} = 0 \\
  \beta_{PHONE} = 0
\end{cases}
\qquad
H_a: \beta_{GAS}^2 + \beta_{PHONE}^2 > 0
\]
Выпишем ограниченную и неограниченную модели:
\begin{multline*}
R: lnPRICE_i = \beta_{Intercept} + \beta_{GENSQUARE} GENSQUARE_i + \varepsilon_i \\
UR: lnPRICE_i = \beta_{Intercept} + \beta_{GENSQUARE} GENSQUARE_i + \beta_{GAS} GAS_i \\
+ \beta_{PHONE} PHONE_i + \varepsilon_i
\end{multline*}
Из условия находим: $RSS_{R} = 23.05$, $RSS_{UR} = 22.494$.

При верной $H_0$ $F \sim F_{2, 128}$ и $F_{crit} \approx 3.8$ ($\alpha = 0.05$).
\[
F_{obs} = \frac{(RSS_R - RSS_{UR}) / q}{RSS_{UR}/(n - k_{UR})} = \frac{(23.05 - 22.494) / 2}{22.494 / (132 - 4)} = 1.58
\]
Так как $F_{obs} < F_{crit}$, нет оснований отвергать $H_0$.
\item Чтобы записать неограниченную модель, введём вспомогательную переменную
\[
d_i = \begin{cases}
1, & \text{если город крупный} \\
0, & \text{иначе}
\end{cases}
\]
Тогда неограниченная модель примет вид:
\begin{multline*}
lnPRICE_i = (\beta_{Intercept} + \Delta_1 di) + (\beta_{GENSQUARE} + \Delta_2 d_i) GENSQUARE_i \\
+ (\beta_{GAS} + \Delta_3 d_i) GAS_i + (\beta_{PHONE} + \Delta_4 d_i) PHONE_i + \varepsilon_i \\
RSS_{UR} = 22.494 + 616.709 = 639.203
\end{multline*}
Выпишем также ограниченную модель:
\begin{multline*}
lnPRICE_i = \beta_{Intercept} + \beta_{GENSQUARE} GENSQUARE_i + \beta_{GAS} GAS_i \\
+ \beta_{PHONE} PHONE_i + \varepsilon_i \\
RSS_{R} = 710.21
\end{multline*}
Проверятеся следующая гипотеза:
\[H_0:
\begin{cases}
  \Delta_1 = 0 \\
  \Delta_2 = 0 \\
  \Delta_3 = 0 \\
  \Delta_4 = 0 \\
\end{cases}
\qquad
H_a: \Delta_1^2 + \Delta_2^2 + \Delta_3^2+ \Delta_4^2 > 0
\]
При верной $H_0$ $F \sim F_{4, 1173}$ и $F_{crit} \approx 2.8$ ($\alpha = 0.05$).
\[
F_{obs} = \frac{(RSS_R - RSS_{UR}) / q}{RSS_{UR}/(n - k_{UR})} = \frac{(710.21 - 639.203)/4}{639.203/(1181-8)} \approx 32.58
\]
Поскольку $F_{obs} > F_{crit}$, основная гипотеза отвергается,
а значит, зависимость нельзя считать единой.
\end{enumerate}
\end{enumerate}

\subsection{Кр 3, 2018-03-28, бп часть}

\subsubsection*{Тест}


% \input{tests/2017_kr_03}

% \AMCnumero{1} % начинаем нумерацию вопросов с 1го

% \cleargroup{all}
% \copygroup[10]{2016_kr_03}{all}
% \insertgroup{all}

\subsubsection*{Задачи}



\begin{enumerate}
\item На основании  наблюдений получена	МНК оценка уравнения регрессии  $\hY_i = 0.2 Z_i + 0.3 W_i$ и оценка дисперсии ошибок $\hat\sigma^2 = 0.04$. Матрица наблюдений регрессоров имеет вид
\[
X^T = \begin{pmatrix}
1 & 2 & 3 & 0 & 0 & 0 \\
0 & 0 & 0 & 4 & 5 & 6 \\
\end{pmatrix}.
\]

Ошибки имеют нормальное распределение.

Постройте 95\% предиктивный интервал (доверительный интервал для индивидуального прогноза) в точке $Z=-2$, $W=5$.


\item В модели множественной регрессии $Y = X\beta + \e$
выполнены все предпосылки классической линейной модели кроме предпосылки о гомоскедастичности. Вектор ошибок имеет нормальное распределение, а возможная гетероскедастичность имеет вид
\[
\Var(\e_i) = \begin{cases}
\sigma^2_1, \text{ при } i\leq m; \\
\sigma^2_2, \text{ при } i > m.\\
\end{cases}
\]

Матрица $X$ имеет размер $n$ на $k+1$.

Выведите формулу статистики LR-теста для проверки гипотезы о гомоскедастичности.

\item Рассмотрим модель $Y_i = \beta X_i + \e_i$, где $\e_i$ — независимые случайные величины с $\E(\e_i)=0$ и $\Var(\e_i)=2018i$.

Найдите наиболее эффективную оценку для параметра $\beta$ в классе всех линейных по $Y$ несмещённых оценок.

\item По 1000 наблюдений  Винни-Пух оценил логистическую модель $\P(Y_i = 1)=F(\beta_0 + \beta_1 X_i)$, где  $X_i$ — количество времени в часах, проведённое в гостях, а $Y_i$ — факт застревания при выходе.

Оценки параметров равны $\hb_0 =2$, $\hb_1 = 3$, с оценкой ковариационной матрицы
\[
\begin{pmatrix}
0.25 & 0.1 \\
0.1 & 0.16 \\
\end{pmatrix}.
\]

\begin{enumerate}
\item Проверьте значимость отдельных коэффициентов при уровне значимости 5\%;
\item Найдите предельный эффект времени, проведённого в гостях, на вероятность застрять при выходе для получасового визита;
\item Найдите максимально возможный предельный эффект.
\end{enumerate}
\end{enumerate}



\subsection{Кр 3, 2018-03-28, бп часть, решения}

\begin{enumerate}
\item Найдём оценку ковариационную матрицу оценок коэффицентов:
\[
\widehat \Var(\hat \beta) = \hat \sigma^2 (X'X)^{-1} = 0.04
\begin{pmatrix}
1/14 & 0 \\
0 & 1/77
\end{pmatrix},
\]
дисперсию ошибки прогноза:
\begin{align*}
\widehat \Var(y_i - \hat y_f | X) &=  \widehat\Var(\beta_z z_i + \beta_w w_i + \varepsilon_i - \hat y_f | X) \\
&= \widehat\Var\left(\varepsilon_i | X\right) + \widehat \Var\left(-2 \hat \beta_z + 5 \hat \beta_w  | X \right) - 2 \widehat \Cov\left(\varepsilon_i, \hat y_f\right) \\
&= 0.04 + 4 \widehat \Var\left(\hat \beta_z | X\right) + 25 \widehat \Var\left(\hat \beta_w | X\right) - 2 \cdot (-2) \cdot 5 \widehat \Cov\left(\hat \beta_z, \hat \beta_w\right) + 0 \\
&= 0.04 + 4 \cdot 0.04 \cdot \frac{1}{14} + 25 \cdot 0.04 \cdot \frac{1}{77} + 0 \\
&\approx 0.0644,
\end{align*}
и сам прогноз:
\[
\hat y_f = 0.2 \cdot (-2) + 0.3 \cdot 5 = 1.1.
\]
Теперь можно выписать доверительный интервал, $t_{0.975, 4} = 2.78$:
\[
\left[1.1 - 2.78 \sqrt{0.0644}; 1.1 + 2.78 \sqrt{0.0644}\right]
\]
\item Функция правдоподобия в неограниченной модели ($\sigma^2_1 \neq \sigma^2_2$)
имеет вид:
\[
L\left(\sigma^2_1, \sigma^2_2\right) = \prod_{i=1}^m \frac{1}{\sqrt{2\pi\sigma^2_1}} e^{-\frac{1}{2}\frac{\left(y_i - x_i'\beta\right)^2}{\sigma^2_1}}
\cdot \prod_{i=m+1}^n \frac{1}{\sqrt{2\pi\sigma^2_2}} e^{-\frac{1}{2}\frac{\left(y_i - x_i'\beta\right)^2}{\sigma^2_2}}
\]
Выпишем также логарифмическую функцию правдоподобия для неограниченной модели
и найдём оценки $\hat \sigma^2_1$, $\hat \sigma^2_2$:
\begin{align*}
\ell\left(\sigma^2_1, \sigma^2_2\right) &= -\frac{m}{2} \ln 2\pi - \frac{m}{2} \ln \sigma^2_1 -
\frac{1}{2\sigma^2_1} \sum_{i=1}^m \left(y_i - x_i'\beta\right)^2 \\
&- \frac{n-m}{2} \ln 2\pi - \frac{n-m}{2} \ln \sigma^2_2 -
\frac{1}{2\sigma^2_2} \sum_{i=m+1}^n \left(y_i - x_i' \beta\right)^2 \to \max_{\sigma^2_1, \sigma^2_2} \\
\frac{\partial \ell}{\partial \sigma^2_1} &= \left. -\frac{m}{2\sigma^2_1} + \frac{1}{2\left(\sigma^2_1\right)^2} \sum_{i=1}^m \left(y_i - x_i'\beta\right)^2 \right|_{\sigma^2_1 = \hat\sigma^2_1} = 0 \\
\frac{\partial \ell}{\partial \sigma^2_2} &= \left. -\frac{n-m}{2\sigma^2_2} + \frac{1}{2\left(\sigma^2_2\right)^2} \sum_{i=m+1}^n \left(y_i - x_i'\beta\right)^2 \right|_{\sigma^2_2 = \hat\sigma^2_2} = 0 \\
\hat\sigma^2_1 &= \frac{\sum_{i=1}^m \left(y_i - x_i'\beta\right)^2 }{m} \\
\hat\sigma^2_2 &= \frac{\sum_{i=m+1}^n \left(y_i - x_i'\beta\right)^2}{n-m}
\end{align*}
Тогда логарифмическая функция правдоподобия примет вид:
\[
\ell_{UR}\left(\hat \sigma^2_1, \hat \sigma^2_2\right) = - \frac{n}{2} \ln 2 \pi - \frac{n}{2} -
\frac{m}{2} \ln \frac{\sum_{i=1}^m \left(y_i - x_i'\beta\right)^2 }{m} - \frac{n-m}{2} \ln \frac{\sum_{i=m+1}^n \left(y_i - x_i'\beta\right)^2}{n-m}
\]
Проделаем то же самое для ограниченной модели ($\sigma^2_1 = \sigma^2_2 = \sigma^2_0$):
\begin{align*}
L\left(\sigma^2_0\right) &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2_0}} e^{-\frac{1}{2} \frac{\left(y_i - x_i'\beta\right)^2}{\sigma^2_0}} \\
\ell\left(\sigma^2_0\right) &= -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2_0 - \frac{1}{2\sigma^2_0} \sum_{i=1}^n \left(y_i - x_i'\beta\right)^2 \to \max_{\sigma^2_0} \\
\frac{\partial \ell}{\partial \sigma^2_0} &= \left. -\frac{n}{2\sigma^2_0} + \frac{1}{2\left(\sigma^2_0\right)^2} \sum_{i=1}^n \left(y_i - x_i'\beta\right)^2 \right|_{\sigma^2_0 = \hat\sigma^2_0} = 0 \\
\hat\sigma^2_0 &= \frac{\sum_{i=1}^n \left(y_i - x_i'\beta\right)^2 }{n} \\
\ell_R\left(\hat \sigma^2_0\right) &= -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \frac{\sum_{i=1}^n \left(y_i - x_i'\beta\right)^2 }{n} - \frac{n}{2}
\end{align*}
Осталось выписать формулу статистики LR-теста:
\begin{align*}
LR &= 2(\ell_{UR} - \ell_{R}) \\
&= 2 \left(-\frac{m}{2} \ln \frac{\sum_{i=1}^m \left(y_i - x_i'\beta\right)^2 }{m} - \frac{n-m}{2} \ln \frac{\sum_{i=m+1}^n \left(y_i - x_i'\beta\right)^2}{n-m}
+ \frac{n}{2} \ln \frac{\sum_{i=1}^n \left(y_i - x_i'\beta\right)^2 }{n} \right)
\end{align*}
\item Наиболее эффективная оценка коэффициента $\beta$ может быть получена
с помощью взвешенного МНК. Для этого необхдимо оценить регрессиию
\[
\frac{y_i}{\sqrt{i}} = \beta \frac{x_i}{\sqrt{i}} + \frac{\varepsilon_i}{\sqrt{i}}.
\]
Переобозначив $\frac{y_i}{\sqrt{i}}$ за $y_i^*$, $\frac{x_i}{\sqrt{i}}$ за
$x_i^*$ и $\frac{\varepsilon_i}{\sqrt{i}}$ за $\varepsilon_i^*$, получим регрессию
\[
y_i^* = \beta x_i^* + \varepsilon_i^*,
\]
применив к которой обычный МНК, получим эффективную оценку $\hat \beta_{WLS}$ вида
\[
\hat \beta_{WLS} = \frac{\sum_{i=1}^n x_i^* y_i^*}{\sum_{i=1}^n (x_i^*)^2} = \frac{\sum_{i=1}^n \frac{x_i}{\sqrt{i}} \cdot \frac{y_i}{\sqrt{i}}}{\sum_{i=1}^n \frac{x_i^2}{i}}.
\]
\item
\begin{enumerate}
\item $\beta_0: \frac{2 - 0}{\sqrt{0.25}} = 4 > 2 \Rightarrow$ гипотеза о незначимости
коэффициента отвергается

$\beta_1: \frac{3 - 0}{\sqrt{0.16}} = 7.5 > 2 \Rightarrow$ гипотеза о незначимости
коэффициента отвергается
\item
\begin{align*}
\frac{\partial \hat\P(Y_i = 1)}{\partial X_i} &= F'\left(\hat \beta_0 + \hat \beta_1 \cdot X_i\right) \cdot \hat \beta_1 \\
&= \left. F\left(\hat \beta_0 + \hat \beta_1 \cdot X_i\right)\left(1 - F\left(\hat \beta_0 + \hat \beta_1 \cdot X_i\right)\right) \cdot \hat\beta_1 \right|_{X_i=0.5} \\
&= \frac{e^{3.5}}{1+e^{3.5}} \left(1 - \frac{e^{3.5}}{1+e^{3.5}} \right) \cdot 3 \\
&\approx 0.085
\end{align*}
\item Предельный эффект максимален в точке, где наклон касательной к логистической функции
самый крутой, то есть в нуле:
\[
2 + 3x = 0 
\]
Формально получается $x = -\frac{2}{3}$, однако в данной задаче $x$ неотрицательная величина, 
поэтому оптимум оказывается в точке $x=0$.
\end{enumerate}
\end{enumerate}




\subsection{КР 3, 2018-03-28, ип часть}


Ровно 42 года назад польская путешественница Кристина Хойновская-Лискевич начала первое женское одиночное кругосветное плавание на парусной яхте.
Плавание продлилось примерно два года.

% На этой контрольной мы постараемся исследовать свойства ML оценок :)


\begin{enumerate}

  \item Рассмотрим задачу линейной регрессии для случая идеально точно известной дисперсии: $y=X\beta + u$, $u\sim \cN(0; I_{n\times n})$,
    где $I_{n\times n}$ — единичная матрица.
    Обозначим $s(\beta)$ — вектор-столбец, градиент логарифмической функции правдоподобия,
    а $\hb$ — оценку параметров $\beta$ с помощью максимального правдоподобия.

    \begin{enumerate}
      \item Найдите $\Var(s(\beta)|X)$ и вспомните $\Var(\hat\beta|X)$;
      \item Упростите выражение $\Var(s(\beta)|X) \cdot \Var(\hat\beta|X)$;
    \end{enumerate}

  \item В созвездии Малой Медведицы водится $k$ видов медведепришельцев.
    Исследователь Миша отлавливает $n$ медведепришельцев и классифицирует их по видам:
    $y_1$ — количество медведепришельцев первого вида, $y_2$ — второго, \ldots, $y_k$ — $k$-го.
    Миша хочет оценить вектор вероятностей $p=(p_1, \ldots, p_{k-1})$.
    Вероятностей на одну меньше, чем видов, чтобы избежать жёсткой линейной зависимости между ними.
    \begin{enumerate}
      \item Как распределена в теории величина $y_1$? Чему равна её дисперсия?
      \item Как распределена в теории величина $y_{12}=(y_1 + y_2)$? Чему равна её дисперсия?
      \item Чему равна ковариация $y_1$ и $y_2$?
      \item Выпишите функцию правдоподобия с точностью до домножения на константу;
      \item Найдите $\hat p_{ML}$;
      \item Найдите $\Var(s(\theta))$ и $\Var(\hat p)$;
      \item Найдите предел $\lim \Var(s(\theta)) \cdot \Var(\hat p)$?
    \end{enumerate}



  \item Исследовательница Несмеяна вывела хитрую формулу для $\hat a$ — несмещённой оценки неизвестного векторного параметра $a$.
    Обозначим $s(a)$ — вектор-столбец градиент логарифмической функции правдоподобия. Докажите, что для оценки Несмеяны выполнено неравенство Крамера-Рао,
    а именно, матрица $M = \Var(s(a))\cdot \Var(\hat a) - I_{k\times k}$ положительна определена.



Подсказки:

    \begin{enumerate}
      \item Вспомните, чему равно $\E(s(a))$. Достаточно просто вспомнить, доказывать не требуется!
      \item Найдите скаляры $\Cov\left(\hat a_1, \frac{\partial \ell}{\partial a_1}\right)$,
	$\Cov\left(\hat a_1, \frac{\partial \ell}{\partial a_2}\right)$
	и матрицу $\Cov\left(\hat a, s(a) \right)$.
      \item Рассмотрим два произвольных случайных вектора $R$ и $S$ и два вектора констант подходящей длины $\alpha$ и $\beta$.
	Найдите минимум функции $f(\alpha, \beta) = \Var(\alpha^T R + \beta^T S)$ по $\beta$.
	Выпишите явно $\beta^*(\alpha)$ и $f^*(\alpha)$.
      \item Докажите, что для произвольных случайных векторов положительно определена матрица
	\[
          \Var(R) - \Cov(R, S) \Var^{-1}(S)\Cov(S, R)
	\]
      \item Завершите доказательство векторного неравенства Крамера-Рао.


    \end{enumerate}

  Без угрызений совести можно храбро переставлять интегралы и производные :)

  \item[3-лайт!] Утешительная версия задачи про Несмеяну. Если не получилось доказать векторную версию неравенства Крамера-Рао, то докажите скалярную :)

    Докажите, что для несмещённой скалярной оценки  $\Var(s(a))\cdot \Var(\hat a) \geq 1$.

    Подсказки:

    \begin{enumerate}

      \item Вспомните, чему равно $\E(\ell'(a))$. Достаточно просто вспомнить, доказывать не требуется!
      \item Найдите $\Cov(\hat a, \ell'(a))$;
      \item Сколько корней может быть у параболы $f(t)=\Var(R + tL)$? Каким может быть дискриминант параболы $f(t)$?
      \item  Докажите для произвольных случайных величин $R$ и $L$ неравенство Коши-Шварца,
	\[
	  \Var(R) \cdot \Var(L) \geq \Cov^2(R, L) .
	\]
    \item Завершите доказательство скалярного неравенства Крамера-Рао :)
    \end{enumerate}


  \item[4.] Идея доказательства состоятельности ML оценки :)

    Пусть наблюдения $y_1$, \ldots, $y_n$ независимы и одинаково распределены с функцией плотности, зависящей от параметра $a$.
    Истинное значение параметра обозначим буквой $a_0$. Оценку максимального правдоподобия обозначим $\hat a$.

    Рассмотрим отмасштабированную логарифмическую функцию правдоподобия $\ell_n(a)=\ell(a) / n$, и
    ожидаемую логарифмическую функцию правдоподобия\footnote{Внимание:
    ожидание считается с помощью истинного $a_0$ от функции, в которую входит константа $a$.},
    $\tilde \ell(a)=\E(\ell(a))$.
    \begin{enumerate}
      \item Что больше, $\ln x$ или $x-1$? Докажите!
      \item В какой точке находится максимум функции $\ell_n(a)$?
      \item В какой точке находится максимум функции $\tilde \ell(a)$?

	Подсказка: рассмотрите выражение $\tilde \ell(a) - \tilde \ell(a_0)$ и примените доказанное неравество :)
      \item К чему сходится $\ell_n(a)$ по вероятности?
      %\item К чему сходится $\ell^{\prime\prime}_n(a)$?

    \end{enumerate}


  \item[5.] Известна структура обратимой матрицы $M$,
    \[
         M = \begin{pmatrix}
	   A & B \\
	   0 & I_{k\times k} \\
	 \end{pmatrix}.
    \]

    \begin{enumerate}
      \item Найдите $M^{-1}$.
      \item Какие условия должны выполняться на блоки $A$ и $B$, чтобы $M$ была обратимой?
    \end{enumerate}


\end{enumerate}


\subsection{Экзамен, 18.06.2018}

\subsubsection*{Тест}


\input{tests/2017_kr_04_final}

\AMCnumero{1} % начинаем нумерацию вопросов с 1го

\cleargroup{all}
\copygroup[10]{2017_fall_retake_1}{all}
\insertgroup{all}

\subsubsection*{Задачи}


\begin{enumerate}
\item Винни-Пух и Пятачок хотят оценить неизвестный параметр $a$ обобщённым методом моментов.
Винни-Пух наблюдает независимые и одинаково распределённые величины $X_i$
с математическим ожиданием $\E(X_i)=a+3$. А Пяточку известны независимые и
одинаково распределённые величины $Y_i$ с ожиданием $\E(Y_i)=a-1$.

По выборке из 100 величин $X_i$ и из 100 величин $Y_i$ оказалось,
что $\sum X_i = 500$ и $\sum Y_i = -50$.

\begin{enumerate}
\item Найдите оценку обобщённого метода моментов для единичной взвешивающей матрицы.
\item Оцените оптимальную взвешивающую матрицу, если дополнительно известно,
что $\Var(X_i)=a^2 + 25$, $\Var(Y_i)=9$, $\Cov(X_i, Y_i)=-4$.
\end{enumerate}

\item Кролик считает, что процесс $Y_i$ подчиняется уравнению:
\[
Y_t = 2 + 0.2 Y_{t-1} + u_t + 0.6 u_{t-1},
\]
где процесс $u_t$ — белый шум c дисперсией $\Var(u_t)=\sigma^2_u$.

\begin{enumerate}
\item Есть ли у этого уравнения стационарное решение (является ли данный процесс стационарным)?
Если да, то найдите для него $\E(Y_t)$ и $\Var(Y_t)$.
%\item Приведите пример нестационарного решения данного уравнения.
\item Постройте 95\%-ый предиктивный интервал для $Y_{102}$, если дополнительно известно,
что $Y_{100}=3$, $u_{100}=-1$, а величины $u_t$ имеют нормальное распределение $\cN(0;16)$.
\end{enumerate}

\item Сова пытается \textit{исследовать} зависимость времени,
проведённого Винни-Пухом \textit{на выходе} из дома Кролика,
от количества съеденных блинов с мёдом и блинов со сгущёнкой.
Она оценила модель $vremya_i = \beta_0 + \beta_1 myod_i + \beta_2 sguschenka_i + u_i$
по всем дням, а также отдельно по выходным и будням.

    \begin{center}
 			\begin{tabular}{c|ccc}
				\toprule
				& Будни & Выходные & Вся выборка \\
				\midrule
                RSS & 500 & 350 & 950 \\
                Наблюдений & 100 & 60 & 160 \\
				\bottomrule
			\end{tabular}
		\end{center}

\begin{enumerate}
\item На уровне значимости 5\% проверьте гипотезу о том, что зависимость не зависит от дня недели.
\item Является ли использованный способ устойчивым к гетероскедастичности?
Если нет, то пошагово опишите способ, устойчивый к гетероскедастичности.
\end{enumerate}


\item
Рассмотрим систему одновременных уравнений

\[
    \begin{cases}
    y_{1t} = \gamma_{10} + \beta_{12} y_{2t} + \beta_{13} y_{3t} + \gamma_{11} x_{1t} + \gamma_{12} x_{2t} + \varepsilon_{1t} \\
    y_{2t} = \gamma_{20} + \beta_{21} y_{1t} + \gamma_{21} x_{1t} + \varepsilon_{2t} \\
    y_{3t} = \gamma_{30} + \beta_{31} y_{1t} + \beta_{32} y_{2t} + \gamma_{31} x_{1t} + \gamma_{33} x_{3t} + \varepsilon_{3t} \\
    \end{cases}
\]

\begin{enumerate}
    \item Идентифицируемо ли каждое из уравнений системы?
    \item К чему приведёт применение к первому уравнению двухшагового метода наименьших квадратов?
\end{enumerate}

\item
Согласно теории Милтона Фридмена, потребление домохозяйства связано не с его общим доходом $Y_t$,
а с его постоянным доходом $Y_t^P$, при этом общий доход может быть представлен
как сумма постоянного дохода и переменного дохода $Y_t^T$ так, что $Y_t = Y_t^P + Y_t^T$.
Переменный доход — случайная величина с нулевым математическим ожиданием и постоянной дисперсией.
Постоянный и переменный доход некоррелированы. Другими словами, потребление пропорционально постоянному доходу $C_t = \beta_1 Y_t^P$,
где $\beta_1$ — предельная склонность к потреблению.

\begin{enumerate}
\item Рассмотрим экономику, где дисперсия переменного дохода составляет $0.5$ от дисперсии постоянного дохода,
а склонность к потреблению равна 0.6. Каким будет значение склонности к потреблению,
полученное на основе «наивной» регрессионной модели зависимости потребления
от общего дохода $C_t = b_0 + b_1 Y_t + \varepsilon_t$?
\item Опишите пошагово способ, каким можно получить состоятельную оценку склонности к потреблению.
\end{enumerate}

% Каково истинное значение склонности к потреблению, котороы Вы бы ожидали увидеть в этой модели?

\end{enumerate}


\subsection{Экзамен, 18.06.2018, решения}

\begin{enumerate}
\item
\begin{enumerate}
\item Выпишем моментные условия:
\begin{align*}
g_1(X_i, a) &= X_i - a - 3 \quad \Rightarrow \quad \bar{g}_1 = \bar X - a - 3 = 2 - a  \\
g_2(Y_i, a) &= Y_i - a + 1 \quad \Rightarrow \quad \bar{g}_2 = \bar Y - a + 1 = 0.5 - a
\end{align*}
Решим задачу минимизации невязки:
\begin{align*}
Q &= \begin{pmatrix}
2 - a &  0.5 - a
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
2 - a \\
0.5 - a
\end{pmatrix} = \\
&= \left(2 - a \right)^2 + \left(0.5 - a \right)^2 \to \min_a \\
\frac{\partial Q}{\partial a} &= \left. -2(2 - a) - 2(0.5 - a) \right|_{a = \hat a} = 0 \\
\hat a &= 1.25
\end{align*}
\item Сначала найдём теоретическую оптимальную взвешивающую матрицу:
\[
W = \Var^{-1}(g) =
\begin{pmatrix}
a^2 + 25 & -4 \\
-4 & 9
\end{pmatrix}^{-1} =
\frac{1}{9(a^2 + 25) - 16}
\begin{pmatrix}
9 & 4 \\
4 & a^2 + 25
\end{pmatrix}
\]
Общий множитель нам не важен, он не влияет на точку оптимума. С точностью до общего множителя найдём оценку матрицы весов:
\[
\widehat W = \propto
\begin{pmatrix}
9 & 4 \\
4 & 1.25^2 + 25
\end{pmatrix}
=
\begin{pmatrix}
9 & 4 \\
4 & 26.5625
\end{pmatrix}
\]
\end{enumerate}
\item
\begin{enumerate}
\item Корень лагового многочлена
\[
1 - 0.2 L = 0
\]
больше единицы, значит, уравнение имеет не заглядывающее в будущее стационарное решение.
Тогда, пользуясь тем, что $\E(Y_t) = \E(Y_{t-1})$, найдём матожидание:
\[
\E(Y_t) = 2 + 0.2\E(Y_{t-1}) \quad \Rightarrow \quad \E(Y_t) = 2.5
\]
Чтобы найти дисперсию, перепишем исходное уравнение с помощью оператора лага:
\begin{align*}
(1 - 0.2 L) Y_t &= 2 + u_t + 0.6 u_{t-1} \\
Y_t &= 2.5 + \frac{1}{1 - 0.2 L} (u_t + 0.6 u_{t-1}) \\
&= 2.5 + u_t + (0.6 + 0.2) u_{t-1} + 0.2(0.6 + 0.2) u_{t-2} + \ldots \\
\Var(Y_t) &= \sigma^2_u \left(1 + (0.6 + 0.2)^2 + 0.2^2 (0.6 + 0.2)^2 + 0.2^4 (0.6 + 0.2)^2 + \ldots\right)^2 \\
&= \sigma^2_u \left(1 +  \frac{(0.6 + 0.2)^2}{1 - 0.2^2} \right) \approx 1.67 \sigma^2_u
\end{align*}
\item Найдём прогноз $Y_{102}$:
\begin{align*}
\E(Y_{102} | Y_{100}, u_{100}) &= \E(2 + 0.2 Y_{101} + u_{102} + 0.6 u_{101} | Y_{100}, u_{100}) \\
&= 2 + 0.2 \E(2 + 0.2Y_{100} + u_{101} + 0.6 u_{100}) \\
&= 2 + 0.2 \cdot 2 + 0.04 \cdot 3 + 0.2 \cdot 0.6 \cdot (-1) = 2.4
\end{align*}
И дисперсию:
\begin{align*}
\Var(Y_{102} | Y_{100}, u_{100}) &= \Var(2 + 0.2 Y_{101} + u_{102} + 0.6 u_{101} | Y_{100}, u_{100}) \\
&= \Var(0.2(2 + 0.2Y_{100} + u_{101} + 0.6 u_{100}) + u_{102} + 0.6 u_{101} | Y_{100}, u_{100}) \\
&= \Var(0.2 u_{101} + u_{102} + 0.6 u_{101}) \\
&= 16 \cdot (0.04 + 1 + 0.36) = 22.4
\end{align*}
Тогда доверительный интервал имеет вид:
\[
\left[2.4 - 1.96 \sqrt{22.4}; 2.4 + 1.96 \sqrt{22.4} \right]
\]
\end{enumerate}
\item
\begin{enumerate}
\item Проверяем гипотезу с помощью F-статистики:
\[
F_{obs} = \frac{(RSS - RSS_1 - RSS_2) / q}{(RSS_1 + RSS_2) / (n - k_{UR})} = \frac{(950 - 500 - 350) / 3}{(500 + 350) / 154} \approx 6
\]
При верной $H_0$ статистика имеет распредление $F_{3, 154}$.
Поскольку $F_{crit} = 2.66 < F_{obs}$, основная гипотеза об одинаковой зависимости
для выходных будней отвергается.
\item
\end{enumerate}
\item
\begin{enumerate}
\item Условие порядка о том, что количество не включённых в правую часть
уравнения переменных должно быть не меньше числа включённых эндогенных, выполняется только
для второго уравнения системы.
Остаётся проверить для второго уравнения условие ранга,
оно также окажется выполнено.
\item К жёсткой мультиколлинеарности на втором шаге.

На первом шаге будут построены регрессии:
\begin{align*}
\hat{y}_{2t} &= \hat{\alpha}_{0} + \hat{\alpha}_1 x_{1t} + \hat{\alpha}_2 x_{2t} + \hat{\alpha}_3 x_{3t} \\
\hat{y}_{3t} &= \hat{\delta}_{0} + \hat{\delta}_1 x_{1t} + \hat{\delta}_2 x_{2t} + \hat{\delta}_3 x_{3t} \\
\end{align*}
На втором шаге строим регрессию
\[
\hat{y}_{1t} = \hat{\gamma}_{01} + \hat{\beta}_{12} \hat{y}_{2t} + \hat{\beta}_{13} \hat{y}_{3t} + \hat{\gamma}_{11} x_{1t} + \hat{\gamma}_{12} x_{12t}
\]
Однако $\hat{y}_{2t}$ и $\hat{y}_{3t}$ являются линейными комбинациями $x_{1t}$,
$x_{2t}$, $x_{3t}$, а значит, в регрессии второго шага есть линейно зависимые регрессоры.
Поэтому МНК-оценки получить нельзя.
\end{enumerate}
\item
\begin{enumerate}
\item Покажем, что полученная оценка будет несостоятельной:
\begin{align*}
\plim \hat \beta_1 &= \plim \frac{\sCov(Y_t, C_t)}{\sVar(Y_t)} = \frac{\Cov(Y^T_t, C_t)}{\Var(Y_t)} \\
&= \frac{\Cov(Y_t^P + Y_t^T, \beta_1 Y_t^P)}{\Var(Y_t^P + Y_t^T)} = \frac{\beta_1 \Var(Y_t^P)}{\Var(Y_t^P) + \Var(Y_t^T)} \\
&= \frac{2}{3} \beta_1
\end{align*}
\item Состоятельную оценку склонности к потреблению можно получить,
если найти такую переменную $z$, что $\Cov(Y^P_t, z) \neq 0$, $\Cov(Y^T_t, z) = 0$.
\end{enumerate}
\end{enumerate}
