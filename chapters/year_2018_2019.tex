% !TEX root = ../metrics_hse_exams.tex

\subsection{ИП, вспомнить всё!}

\begin{enumerate}

  \item Сфорулируйте теорему о трёх перпендикулярах и обратную к ней. Нарисуйте картинку.

  \item Для матрицы
$
  A=\begin{pmatrix}
  4 & 5  \\
  5 & 4  \\
  \end{pmatrix}
$

  \begin{enumerate}
  \item Найдите собственные числа и собственные векторы матрицы;
  \item Найдите определитель $\det A$ и след $\tr A$;
 \item Известно, что $B = A^{-1} + 2018I$, где $I$ — единичная матрица.
 Найдите собственные числа $B$, определитель $\det B$ и след $\tr B$.

  \end{enumerate}


  \item Блондинка Маша встретила 100 динозавров.
  Средний рост динозавров оказался равен 20 метров, а выборочное стандартное отклонение — 5 метров.

  \begin{enumerate}
    \item Постройте 95\% доверительный интервал для математического ожидания роста динозавра.
    \item На уровне значимости 1\% проверьте гипотезу о том, что математическое ожидание
    роста равно 22 метрам. Против альтернативной гипотезе о неравенстве.
    \item Укажите $P$-значение для теста в предыдущем пункте.
  \end{enumerate}

 \item На брег выходят один за одним 33 богатыря. Двадцать вторым по счёту выходит
 богатырь Мефодий. Какова вероятность того, что Мефодий окажется вторым по силе из всех богатырей,
   если известно, что он самый сильный из всех вышедших до него?

\end{enumerate}


\subsection{Вспомнить всё, ответы}

\begin{enumerate}
\item[2.]
\begin{enumerate}
  \item $\lambda^A_1 = -1$, $\lambda^A_2 = 9$,
  $h_1 = \begin{pmatrix}
  1 & -1
  \end{pmatrix}^T$,
  $h_2 = \begin{pmatrix}
  1 & 1
  \end{pmatrix}^T$
  \item $\det(A) = \lambda^A_1 \cdot \lambda^A_2 = -9$, $\tr(A) = \lambda^A_1 +
  \lambda^A_2 = 8$
  \item $\lambda^B_1 = 1 / \lambda^A_1 + 2018 = 2017$,
  $\lambda^B_2 = 1 / \lambda^A_2 + 2018 = 2018 + 1/9$,
  $\det(B) = \lambda^B_1 \cdot \lambda^B_2 = 2017 \cdot (2018 + 1/9)$,
  $\tr(B) = \lambda^B_1 + \lambda^B_2 = 4035 + 1/9$
\end{enumerate}
\item[3.]
\begin{enumerate}
\item $\left[20 - 1.96 \cdot 5 / \sqrt{100}; 20 + 1.96 \cdot 5 / \sqrt{100} \right]$
\item $z_{obs} = -4$, $z_{crit} = \pm 2.6$, основная гипотеза отвергается
\item $p-value \approx 0 $
\end{enumerate}
\item[4.] Заметим, что неважно, каким идёт Мефодий, а главное, что он самый сильный из
$22$ вышедших. Из $22$ вышедших всё равно есть кто-то самый сильный, и если его
считать Мефодием, то ничего не изменится. Значит, нам нужна вероятность того,
что второй лучший из всех попадёт на $22$ места из $33$, а самый лучший на $11$
мест из $32$ оставшихся. Искомая вероятность — $11/48$.

Или по формуле условной вероятности. Пусть $A$ означает, что Мефодий второй по силе
из всех, а $B$ — что он первый по силе из вышедших. Тогда
\[
\P(B) = 1/22,
\]
так как Мефодий должен быть самым сильным из вышедших, и
\[
\P(A \cap B) = 11/33 \cdot 1/32,
\]
так как на $22$-ом месте должен быть самый сильный из вышедших и второй по силе из
всех. Если он второй по силе из вышедших, то первый оказался среди $11$ невышедших,
и эта вероятность равна $11/33$, а вероятность быть вторым по силе среди всех равна
$1/32$. Итого,
\[
\P(A|B) = \frac{\P(A \cap B)}{\P(B)} = \frac{11}{48}.
\]
\end{enumerate}



\subsection{Задачи миниконтрольных ИП}

\begin{enumerate}
  \item Найдите SVD-разложение матрицы $
  \begin{pmatrix}
  2 & 0 & -1 \\
  2 & 1 & 0 \\
  \end{pmatrix}$
 \item Найдите дифференциал $d \cos(r^TAr+br)$, где $A^T=A$ и $b$ — это константы.
 \item Постройте регрессию вектора $y = (4,2,-2)^T$ на вектора $x=(1,0,-1)^T$ и $z=(1,1,-1)^T$
 без константы.
 \item Известно, что $y=x + 2z$. Винни-Пух построил регрессию $\hat y_i = \hat\beta_1 + 0.16 x_i$.
 Пятачок построил регрессию $\hat x_i = \hat \alpha_1 + 1\cdot y_i$.

 Помогите Сове найти коэффициент $\hat \gamma_2$ в регрессии $\hat y_i = \hat\gamma_1 + \hat\gamma_2 z_i$.


\item Величины $U_1$ и $U_2$ независимы и равномерны $U[0;1]$. Рассмотрим пару величин $Y_1 = R\cdot \cos \alpha$, $Y_2 = R\cdot \sin \alpha$, где $R=\sqrt{-2\ln U_1}$, а $\alpha = 2\pi U_2$.
\begin{enumerate}
  \item Выпишите дифференциальную форму для пары $U_1$, $U_2$;
  \item Выпишите дифференциальную форму для пары $Y_1$, $Y_2$;
  \item  Найдите совместный закон распределения $Y_1$ и $Y_2$;
  \item Верно ли, что $Y_1$ и $Y_2$ независимы?
  \item  Как распределены $Y_1$ и $Y_2$ по отдельности?

\end{enumerate}

 \item Спроецируйте вектор $y=(y_1, y_2, y_3, y_4, y_5)'$ на линейную оболочку векторов
 $a=(1, 0, 1,1, 1)'$, $b=(2,1,1,1,1)'$ и $c=(0,3,1,1,1)'$. Выпишите явно квадрат длины
 проекции. Как распределён квадрат длины проекции, если компоненты вектора $y$
 независимы и стандартно нормально распределены.

 \item Докажите эффективность МНК-оценок в задаче множественной регрессии.


\end{enumerate}


\subsection{Контрольная работа-1. Базовая часть}

\input{tests/2018_2019/kr1/tex/exercise01}
\input{tests/2018_2019/kr1/tex/exercise02}
\input{tests/2018_2019/kr1/tex/exercise03}
\input{tests/2018_2019/kr1/tex/exercise04}
\input{tests/2018_2019/kr1/tex/exercise05}
\input{tests/2018_2019/kr1/tex/exercise06}
\input{tests/2018_2019/kr1/tex/exercise07}
\input{tests/2018_2019/kr1/tex/exercise08}
\input{tests/2018_2019/kr1/tex/exercise09}
\input{tests/2018_2019/kr1/tex/exercise10}


\begin{enumerate}
  \item (5 баллов) Случайные величины $X$ и $Y$ независимы и имеют хи-квадрат распределение
  с 5 и с 10 степенями свободы, соответственно. Случайная величина $Z$ равна $Z = (X+Y)/X$.

  Найдите значение $z^*$ такое, что $\P(Z > z^*)=0.05$.
  \item (5 баллов) Докажите, что для модели парной регрессии $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$,
оцененной с помощью МНК, выполнено равенство $\sum_{i=1}^n Y_i = \sum_{i=1}^n \hat Y_i$.

  \item (5 баллов) Аккуратно сформулируйте теорему Гаусса-Маркова для случая парной регрессии.

  \item (10 баллов) На основании 62 наблюдений Чебурашка оценил функцию спроса на апельсины:

 \[
 \hat Y_i = \underset{(1.6)}{3} - \underset{(0.2)}{1.25} X_i, \text{ где } \sum_i (X_i - \bar X)^2 =2.25
 \]

 В скобках приведены стандартные ошибки коэффициентов, случайные ошибки в регрессии можно считать нормальными.


  \begin{enumerate}
    \item Проверьте гипотезы о значимости каждого из коэффициентов регрессии при уровне значимости 5\%.
    \item Проверьте гипотезу о равенстве коэффициента наклона -1 при уровне значимости 5\%
    и односторонней альтернативной гипотезе, что коэффициент наклона меньше -1.
    \item Найдите оценку дисперсии ошибок.
    \item Найдите 95\% интервальный индивидуальный прогноз в точке $X=8$.
  \end{enumerate}
\end{enumerate}

\subsection{Контрольная работа-1. Базовая часть, решения}

\begin{enumerate}
\item Распишем вероятность $\P(Z >z^*)$:
\begin{align*}
\P(Z >z^*) &= \P \left(\frac{X+Y}{X} > z^* \right) = \P \left(1 + \frac{Y}{X} > z^* \right) \\
&= 1 - \P\left(\frac{Y}{X} \leq z^* - 1 \right) = 1 - \P\left(\frac{Y/10}{X/5} \leq \left(z^* - 1\right) \cdot \frac{5}{10} \right)
\end{align*}
Заметим, что
\[
W = \frac{Y/10}{X/5} \sim F_{10,5}
\]
Подставив случайную величину $W$ в выражение для $\P(Z >z^*)$, получим:
\[
\P\left(W \leq 0.5(z^* - 1) \right) = 0.95 \Rightarrow 0.5(z^* - 1) = 4.74 \Rightarrow z^* = 10.48
\]
\item Запишем $Y_i$ через оценку $\hat Y_i$ и остатки $e_i = Y_i - \hat Y_i$ и
просуммируем обе части выражения:
\begin{align*}
Y_i &= \hat Y_i + e_i \\
\sum_{i=1}^n Y_i &= \sum_{i=1}^n \hat Y_i + \sum_{i=1}^n e_i
\end{align*}
Нужно показать, что последнее слагаемое в полученном равенстве равно нулю:
\[
\sum_{i=1}^n e_i = \sum_{i=1}^n Y_i - n\hb_0 - \hb_1 \sum_{i=1}^n X_i
\]
Разделим обе части на $n$:
\[
\frac{1}{n} \sum_{i=1}^n e_i = \bar Y - \hb_0 - \hb_1 \bar X
\]
Осталось вспомнить, что $\hb_0 = \bar Y - \hb_1 \bar X$.
\item Если для модели $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, i=1, \ldots, n$
выполнены условия о том, что
\begin{enumerate}
  \item модель правильно специфицирована,
  \item $X_i$ являются детерминированными величинами и не равны между собой,
  \item $\E(\varepsilon_i) = 0 \quad \forall i$,
  \item $\Var(\varepsilon_i) = \sigma^2_{\varepsilon} \quad \forall i$,
  \item $\Cov(\varepsilon_i, \varepsilon_j) = 0 \quad \forall i \neq j$,
\end{enumerate}
то МНК-оценки $\hb_0$, $\hb_1$ являются лучшими линейными несмещёнными оценками.
\item
\begin{enumerate}
\item Для проверки гипотезы о значимости отдельного коэффициента используем $t$-тест:
\[
t_{obs} = \frac{\hb - \beta}{\hat{\sigma}_{\hb}} \sim t_{n-k}
\]
\begin{itemize}
\item для $\beta_0$: $t_{obs} = 1 / 1.6 = 0.625 < 2 = t_{crit} \Rightarrow$
нет оснований отвергать $H_0$.
\item для $\beta_1$: $t_{obs} = -1.25 / 0.2 = 6.25 > 2 = t_{crit} \Rightarrow$
$H_0$ отвергается.
\end{itemize}
\item Необходимо проверить следующую гипотезу:
\[
\begin{cases}
H_0: \beta_1 = -1 \\
H_a: \beta_1 < -1
\end{cases}
\]
Проверяем:
\[
t_{obs} = \frac{-1.25 - (-1)}{0.2} = -1.25 > -1.67 = t_{crit}
\]
Нет оснований отвергать нулевую гипотезу.
\item Из соотношения для оценки дисперсии $\hb_1$ получим:
\begin{align*}
\hat{\sigma}^2_{\hb_1} &= \frac{\hat{\sigma}^2_{\varepsilon}}{\sum_{i=1}^{62}(X_i - \bar X)^2} \Rightarrow \\
\hat{\sigma}^2_{\varepsilon} &= \hat{\sigma}^2_{\hb_1} \cdot \sum_{i=1}^{62}(X_i - \bar X)^2 \\
&= (0.2)^2 \cdot 2.25 = 0.09
\end{align*}
\item Найдём точечный прогноз в точке $X = 8$:
\[
\hat Y_f = 3 - 1.25 \cdot 8 = -7
\]
Найдём оценку дисперсии прогноза:
\begin{align*}
\widehat{\Var}(\hat Y_f) &= \widehat{\Var} \left(\hb_0 + \hb_1 \cdot 8 \right) \\
&= \widehat{\Var} \left(\hb_0\right) + 64 \widehat{\Var} \left(\hb_1 \right) + 2 \cdot 8 \widehat{\Cov}(\hb_0, \hb_1) \\
&= 1.6^2 + 64 \cdot 0.2^2 + 16 \cdot \frac{-\bar X \hat{\sigma}^2_{\varepsilon}}{\sum_{i=1}^{62}\left(X_i - \bar X\right)^2} \\
&= 1.6^2 + 64 \cdot 0.04 - 16 \cdot \frac{\sqrt{3.96} \cdot 0.09}{2.25} \\
&\approx 3.9
\end{align*}
Осталось выписать доверительный интервал:
\[
\left[-7 -2 \cdot \sqrt{3.9}; -7 + 2 \cdot \sqrt{3.9} \right]
\]
\end{enumerate}
\end{enumerate}


\subsection{Контрольная работа-1. ИП часть}

\begin{enumerate}
  \item Храбрый исследователь Вениамин поделил выборку на обучающую $(X, y)$ и тестовую $(X_{test}, y_{test})$.
  Регрессоры $X$ и $X_{test}$ Вениамин считает нестохастическими, а предпосылки
  теоремы Гаусса-Маркова — выполненными на всей исходной выборке. Естественно,
  $\hat y_{test} = X_{test}\hat\beta$, где $\hat\beta$ оценивается по обучающей выборке.

  Помогите Вениамину найти $\Var(\hat y_{test})$ и $\Cov(\hat \beta, \hat y_{test})$.

  \item Рассмотрим матрицу $X$ полного ранга с $n$ наблюдениями и $k$ столбцами.
  В каких границах могут лежать диагональные элементы матрицы-шляпницы $H$?
  Чему равно их среднее значение?

  Подсказка: найдите $\Var(\hat y)$ и $\Var(\hat u)$ в рамках предпосылок теоремы Гаусса-Маркова.

  \item Рассмотрим стандартный $t$-тест на равенство некоторого коэффициента бета нулю.
  Докажите, что
  \[
         t^2 = \frac{RSS_r - RSS_{ur}}{RSS_{ur}/(n-k)},
  \]
  где $RSS_r$ — сумма квадратов остатков в модели без тестируемого коэффициента
  (выкинут регрессор при проверямом коэффициенте),
  $RSS_{ur}$ — аналогичная сумма в модели с включённым тестируемым коэффициентом, $k$ —
  число оцениваемых коэффициентов бета в модели с тестируемым коэффициентом, $n$ —
  количество наблюдений.

  Утешительный приз: упростите эту формулу для случая парной регрессии и докажите её :)

  \item Рассмотрим стандартную ошибку оценки коэффициента бета при регрессоре $z$
  в множественной регрессии.
  Докажите, что

  \[
         se^2(\hat\beta_z) = \frac{RSS / (n-k)}{\sum (z_i - \bar z)^2} \cdot \frac{1}{1 - R^2_z},
  \]
  где $R^2_z$ — коэффициент детерминации во вспомогательной регресии объясняющей переменной
  $z$ на остальные объясняющие переменные.

  Утешительный приз: упростите эту формулу для случая парной регрессии и докажите её :)


   \item У Винни-Пуха есть случайный вектор $w$ и одномерная случайную величину $z$.
   Винни-Пуху известны величины $\Cov(w, w) = A$ и $\Cov(w, z) = b$.

   К сожалению, у Винни-Пуха опилки в голове, а он очень хочет найти такую линейную комбинацию
   компонент вектора $w$, которая была бы сильнее всего коррелирована со случайной
   величиной $z$.

   Помогите Винни-Пуху!

   Как выглядят веса этой линейной комбинации?
   Чему равна максимально возможная корреляция?

 \item Машенька построила парную регрессию по 11 наблюдениям с $R^2=
0.95$. Чтобы напакостить Машеньке, Вовочка переставил в случайном
порядке значения зависимой переменной и предложил Машеньке заново оценить модель.

Какой ожидаемый $R^2$ получит Машенька?
\end{enumerate}


\subsection{ИП-часть, решения}
\begin{enumerate}
\item Дисперсия:

\begin{align*}
\Var \left(\hat{y}_{test}\right) &= \Var \left(X_{test} \hat\beta \right) \\
&= X_{test} \Var\left((X'X)^{-1} X' y \right) X'_{test} \\
&= X_{test} (X'X)^{-1} X' \sigma^2 \cdot I X (X'X)^{-1}  X'_{test} \\
&= \sigma^2 X_{test} (X'X)^{-1} X'_{test}
\end{align*}

Ковариация:
\begin{align*}
\Cov\left(\hat \beta, \hat{y}_{test}\right) &= \Cov\left(\hat \beta,  X_{test} \hat{\beta} \right) \\
&= \Var\left(\hat \beta\right) X'_{test} \\
&= \Var\left((X'X)^{-1} X' y \right)X'_{test} \\
&= (X'X)^{-1} X' \sigma^2 \cdot I X (X'X)^{-1}  X'_{test} \\
&= \sigma^2 (X'X)^{-1} X'_{test}
\end{align*}

\item
Воспользуемся подсказкой и найдём:
\begin{align*}
\Var\left(\hat y\right) &= \Var \left(Hy \right) = H \Var(y) H' = \sigma^2 H \\
\Var \left(\hat u \right) &= \Var \left(y - \hat y\right) = \Var \left((I-H)y\right) = \sigma^2 (I-H)
\end{align*}


Соответственно, для диагональных элементов выполнятются соотношения:
\[
\Var\left(\hat y_i \right) = \sigma^2 h_{ii} \text{ и } \Var \left(\hat u_i \right) = \sigma^2(1 - h_{ii}),
\]
откуда следует, что диагональные элементы матрицы-шляпницы лежат в пределах от $0$ до $1$.

Вспомним, что сумму диагональных элементов, или след матрицы, можно вычислить
как сумму её собственных значений.
Поскольку матрица-шляпница является проектором, её собственные числа равны либо
 $0$, либо $1$.
Это легко показать. Пусть $\lambda$ — собственное значение матрицы $H$ с собственным
вектором $v$. Для $H$ выполняется соотношение $H^2 = H$, а значит верно и
\[
\lambda^2 v = H^2 v = H v = \lambda v,
\]
где $v \neq 0$. Отсюда получаем уравнение $\lambda^2 = \lambda$, корни которого —
$0$ и $1$.

Осталось заметить, что для собственных чисел $\lambda = 1$ собственные вектора
лежат в $Lin(X)$, а для нулевых собственных чисел — перпендикулярны ей.
Значит, количество единиц совпадает с размерностью пространства, на которое проецируем.
В нашем случае оно равно $k$. Значит, среднее диагональных элементов — $k/n$.

\item Утверждение можно доказать геометрически.

\begin{align*}
\frac{t}{\sqrt{n-2}} &=
\frac{\hat \beta_2}{\sqrt{n-2}se\left(\hat\beta_2\right)} =
\frac{\hat \beta_2}{\sqrt{n-2}\frac{\hat \sigma}{\sqrt{\sum\limits_{i=1}^n (x_i - \bar x)^2}}}
= \frac{\hat \beta_2 \sqrt{\sum\limits_{i=1}^n (x_i - \bar x)^2}}{\sqrt{n-2}\frac{\sqrt{\sum\limits_{i=1}^n (y_i - \hat y_i)^2}}{\sqrt{n-2}}} \\
&= \frac{\hat \beta_2 \lVert x^c \rVert}{\sqrt{RSS}} = \ctg \varphi\\
\end{align*}

\begin{figure}[ht!]
\begin{center}
\subfigure[]{
\includegraphics[width=0.35\linewidth]{figures/04_ttest.pdf}
\label{fig:ttest_3d}}
%\hspace{4ex}
\subfigure[]{
\includegraphics[width=0.35\linewidth]{figures/04_ttest_lin.pdf}
\label{fig:ttest_lin}}
\caption{\subref{fig:ttest_3d}: Регрессия $y$ на $\Lin(x, \mathbf{1})$ и соответствующие проекции;
\subref{fig:ttest_lin}: $\Lin^{\perp}(\mathbf{1})$.}
\end{center}
\end{figure}

\[
F = \frac{(RSS_{R} - RSS_{UR})/q}{RSS_{UR}/(n-k_{UR})} =
\ctg^2 \varphi \cdot \frac{n - k_{UR}}{q}
\]

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.55]{figures/04_ftest.pdf}
\caption{F-статистика пропорциональна квадрату котангенса $\varphi$,
где $a$ означает $\sqrt{RSS_{UR}}$, $b$ — $\sqrt{RSS_{R} -RSS_{UR}}$,
$c$ — $\sqrt{RSS_{R}}$.}
\label{fig:ftest}
\end{figure}

В случае парной регрессии $k_{UR} = 2, q = 1$.

\item Представим матрицу регрессоров в виде двух блоков: $X = [Z \quad W]$, где $Z$ —
это регрессор (вектор), для которого мы будем искать стандартную ошибку, а
$W$ — это матрица всех остальных регрессоров.
Известно, что
\[
\Var\left(\hat{\beta}_{Z} \right) = \sigma^2 (X'X)^{-1}_{11},
\]
то есть нам нужно найти только первый элемент матрицы $X'X$.
Эта матрица является блочной и в наших обозначениях имеет вид:
\[
X'X = \begin{pmatrix}
Z'Z & Z'W \\
W'Z & W'W
\end{pmatrix}
\]

Чтобы найти обратную матрицу в общем случае, нужно решить систему:
\[
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}
\cdot
\begin{pmatrix}
E & F \\
G & H
\end{pmatrix}
=
\begin{pmatrix}
I & 0 \\
0 & I
\end{pmatrix},
\]
где матрица слева изветсна, матрица из блоков $E, F, G, H$ является обратной к ней.
Решая эту систему, например, методом Гаусса, получим:
\[
E = (A-BD^{-1}C)^{-1},
\]
а остальные элементы нас не интересуют.
Возвращаясь к нашим обозначениям, запишем:
\[
(X'X)^{-1}_{11} = \left(Z'Z - Z'W \left(W'W \right)^{-1} W'Z \right)
\]
Обозначив $\left(W'W \right)^{-1} W'= H$ (проектор во вспомогательной регрессии на все
переменные, кроме Z), упростим полученное выражение:
\[
(X'X)^{-1}_{11} = Z'(I-H)Z
\]
Как промежуточный итог мы получили:
\[
\Var\left(\hat{\beta}_{Z} \right) = \frac{\sigma^2}{Z'(I-H)Z}
\]
Теперь рассмотрим коэффициент детерминации во вспомогательной регрессии:
\[
1 - R^2_Z = \frac{RSS_Z}{TSS_Z} = \frac{(Z - \hat Z)'(Z - \hat Z)}{(Z - \bar Z)'(Z - \bar Z)} = \frac{((I-H)Z)'(I-H)Z}{(Z - \bar Z)'(Z - \bar Z)}
= \frac{Z'(I-H)Z}{(Z - \bar Z)'(Z - \bar Z)}
\]
Выражая отсюда числетель, получаем:
\[
Z'(I-H)Z = \left( 1 - R^2_Z \right) \cdot \sum (Z_i - \bar Z)^2
\]
Осталось вспомнить оценку для $\sigma^2$ и записать финальный резульат:
\[
\widehat{\Var}\left(\hat{\beta}_{Z} \right) = \frac{RSS/(n-k)}{\sum (Z_i - \bar Z)^2} \cdot \frac{1}{1 - R^2_Z}
\]

\item В том виде, в котором задача дана, она не решается.
Однако можно было выписать целевую функцию и взять её дифференциал:
\[
\Corr(\alpha^T w, z) = \frac{\Cov(\alpha^T w, z)}{\sqrt{\Var(\alpha^T w)\Var(z)}}
= \frac{\alpha^T \Cov(w,z)}{\sqrt{\alpha^T\Var(w) \alpha \Var(z)}} \to \max_{\alpha}
\]
Также можно было заметить, что эта задача напоминает МНК.
Пусть $\hat z$ — проекция $z$ на $\Lin(w_1, \ldots, w_k)$,
а $\tilde{z}$ — произвольный вектор из $\Lin(w_1, \ldots, w_k)$.
Обозначив угол между $z$ и $\hat z$ за $\alpha$, между $z$ и $\tilde z$ за $\beta$,
можем записать:
\[
\Corr(z, \hat z) = \cos \alpha \geq \cos \beta = \Corr(z, \tilde z)
\]
при $\alpha \leq \beta \leq 90^{\circ}$.
И задача максимизации $\Corr(z, \tilde z)$ будет эквивалентна задаче МНК.

\item Без ограничения общности центрируем исходные переменные.
Значения регрессора обозначим $x_i$, исходную зависимую переменную $\tilde{y}_i$,
а переставленную в случайном порядке $y_i$.
По условию, $\sum y_i^2 = \sum \tilde{y}_i^2$.

Фиксируем исходные переменные и находим:
\[
\E(R^2 | \tilde{y}) = \E\left( \frac{(\sum x_i y_i )^2}{\sum x_i^2 \sum y_i^2}  \right) =
\frac{\Var\left(\sum x_i y_i | \tilde{y}\right)}{\sum x_i^2 \sum y_i^2}
\]

Для начала найдём дисперсию отдельного игрека:
\[
\Var(y_i | \tilde{y}) = \frac{\sum \tilde{y}_i^2}{n} = \frac{\sum y_i^2}{n}
\]

А теперь из $\Cov(y_1, \sum y_i | \tilde{y}) =0$ найдём и дисперсию нужной суммы:
\[
\Var\left(\sum x_i y_i | \tilde{y}\right) =
\Var(y_i | \tilde{y}) \left(  \sum x_i^2 - \sum_{i\neq j} x_i x_j \frac{1}{n-1}  \right) =
\Var(y_i | \tilde{y}) \frac{n}{n-1} \sum x_i^2 = \frac{\sum x_i^2 \sum y_i^2 }{n-1}
\]

Заканчиваем подсчёт,
\[
\E(R^2 | \tilde{y}) = \frac{1}{n-1}
\]

Следовательно, и $\E(R^2) = \frac{1}{n-1} = 0.1$.

\end{enumerate}


\subsection{Контрольная работа 2. 26-12-2018}

\subsubsection*{Тест}

\input{tests/2018_2019/midterm/tex_02/exercise01}
\input{tests/2018_2019/midterm/tex_02/exercise02}
\input{tests/2018_2019/midterm/tex_02/exercise03}
\input{tests/2018_2019/midterm/tex_02/exercise04}
\input{tests/2018_2019/midterm/tex_02/exercise05}
\input{tests/2018_2019/midterm/tex_02/exercise06}
\input{tests/2018_2019/midterm/tex_02/exercise07}
\input{tests/2018_2019/midterm/tex_02/exercise08}
\input{tests/2018_2019/midterm/tex_02/exercise09}
\input{tests/2018_2019/midterm/tex_02/exercise10}

\subsubsection*{Задачи}
\begin{enumerate}
\item
(5 баллов)
Рассмотрим алгоритм LASSO с параметром регуляризации $\lambda$
для модели $Y=X\beta + \varepsilon$, где все переменные центрированы.
\begin{enumerate}
    \item Выпишите целевую функцию алгоритма.
    \item Что произойдет с оценками $\hat\beta_{LASSO}$ при $\lambda \to \infty$?
    \item Что произойдет с оценками $\hat\beta_{LASSO}$ при $\lambda \to 0$?
\end{enumerate}
\item
(5 баллов)
По 200 фирмам была оценена зависимость выпуска $Y$ от труда $L$ и капитала $K$
с помощью двух моделей:

Модель Кобба-Дугласа: $\ln{Y_i} = \beta_0 + \beta_1 \ln{L_i} + \beta_2 \ln{K_i} +
\varepsilon_i$

Транслоговая модель: $\ln{Y_i} = \gamma_0 + \gamma_1 \ln{L_i} + \gamma_2 \ln{K_i} +
\gamma_3 (0.5 \ln^2{L_i}) + \gamma_4 (0.5 \ln^2{K_i}) + \gamma_5 \ln{K_i} \ln{L_i} +
\varepsilon_i$

Оценки коэффициентов обеих моделей (в скобках приведены стандартные ошибки):

\begin{tabular}{lcc}
\toprule
Переменная & Модель Кобба-Дугласа & Транслоговая модель \\
\midrule
константа & $1.1706$ ($0.326$) & $0.9441$ ($2.911$)   \\
$\ln L$ & $0.6029$ ($0.125$) & $3.613$ ($1.548$)  \\
$\ln K$ & $0.375$ ($0.085$) & $-1.893$ ($1.016$)  \\
$0.5 \ln^2 L$ &  & $-0.964$ ($0.707$)  \\
$0.5 \ln^2 K$ & & $0.0852$ ($0.2922$) \\
$\ln L \ln K$ & & $0.3123$ ($0.4389$)  \\
$R^2$ & $0.9$ & $0.954$  \\
\bottomrule
\end{tabular}

В модели Кобба-Дугласа $\hCov(\hb_1, \hb_2)= -0.0096$.

На уровне значимости $\alpha = 0.05$ проверьте следующие гипотезы:
\begin{enumerate}
\item В модели Кобба-Дугласа эластичность выпуска по капиталу равна единице.
\item В модели Кобба-Дугласа эластичности выпуска по труду и капиталу одинаковы.
\item В транслоговой модели $\gamma_3 = 0$.
\item В транслоговой модели $\gamma_3 = \gamma_4 = \gamma_5 = 0$.
\end{enumerate}

\item
(4 балла)
Исследователь оценил зависимость продолжительности жизни $Y$ от концентрации
промышленных выбросов в атмосфере $X$ и ежегодных частных расходов на медицинскую
помощь $Z$.

Для $300$ жителей индустриальных центров, $\hat{Y}_i = \underset{(10.43)}{65.91} -
\underset{(0.0001)}{0.03}X_i - \underset{(0.019)}{0.036}Z_i, \; RSS = 300$.

Для $200$ сельских жителей, $\hat{Y}_i = \underset{(15.3)}{58.4} -
\underset{(0.006)}{0.017}X_i - \underset{(0.007)}{0.024}Z_i, \; RSS = 200$.

А также по общей выборке, $\hat{Y}_i = \underset{(12.4)}{63.2} -
\underset{(0.005)}{0.02}X_i - \underset{(0.001)}{0.031}Z_i, \; RSS = 900$.

В скобках приведены стандартные ошибки.

Можно ли считать, что зависимость едина для городских и сельских жителей?
Ответ обоснуйте подходящим тестом, аккуратно выписав тестируемую гипотезу.

\item
(5 баллов)
Исследователь Д'Артаньян стандартизировал (центрировал и нормировал) все
имеющиеся регрессоры и поместил их в столбцы матрицы $\tilde X$. Выборочная
корреляционная матрица регрессоров равна:
\[
\begin{pmatrix}
1 & 0.85 & 0  \\
0.85 & 1 & 0  \\
0 & 0 & 1 \\
\end{pmatrix}.
\]
\begin{enumerate}
    \item Найдите параметр обусловленности (condition number) матрицы
    $\tilde X^T \tilde X$.
    \item Вычислите одну или две главные компоненты, объясняющие не менее
    $70$\% суммарной дисперсии стандартизированных регрессоров. Выпишите
    найденные компоненты как линейные комбинации столбцов матрицы $\tilde X$.
\end{enumerate}

\item
(6 баллов)
Для $400$ голландских магазинов модной одежды с помощью трёх моделей оценили
зависимость продаж в расчете на квадратный метр в гульденах, $Sales$, от:
\begin{itemize}
\item общей площади магазина, $Size$, в м$^2$;
\item количества сотрудников, работающих целый день, $Nfull$;
\item количества временных рабочих, $Ntemp$;
\item дамми-переменной $Owner$, равной единице, если собственник один, и нулю иначе.
\end{itemize}

$\widehat{Sales}_i = \underset{(718)}{6083} - \underset{(1.59)}{15.25}Size_i +
\underset{(171)}{1452.8} Nfull_i + \underset{(423)}{420.15} Ntemp_i -
\underset{(361)}{1464.1} Owner_i$

$\ln \widehat{Sales}_i = \underset{(0.11)}{8.59} - \underset{(0.00024)}{0.0024}Size_i +
\underset{(0.026)}{0.183} Nfull_i + \underset{(0.066)}{0.102} Ntemp_i -
\underset{(0.056)}{0.209} Owner_i$

$\ln \widehat{Sales}_i = \underset{(0.21)}{10.08} - \underset{(0.043)}{0.31}\ln Size_i +
\underset{(0.061)}{0.22} \ln Nfull_i + \underset{(0.118)}{0.066} \ln Ntemp_i -
\underset{(0.059)}{0.19} \ln Owner_i$

В скобках приведены стандартные ошибки.

\begin{enumerate}
    \item Дайте интерпретацию коэффициента при переменной $Size$ в каждой из трёх моделей;
    \item Подробно опишите, как выбрать наилучшую из этих моделей.
\end{enumerate}
\end{enumerate}


\subsection{Контрольная работа 2. 26-12-2018, решения}

\begin{enumerate}
\item
\begin{enumerate}
  \item $\sum_{i=1}^n \left(Y_i - \hat Y_i \right)^2 + \lambda \sum_{i=1}^k
  \vert \hb_i \vert$
  \item Вес второго слагаемого в целевой функции будет выше, и коэффициенты будут
  зануляться.
  \item Оценки коэффициентов будут стремиться к тем, которые были бы получены
  при обычном МНК.
\end{enumerate}
\item
\begin{enumerate}
  \item $t_{obs} = (0.375 - 1) / 0.085 \approx -7.35$, $t_{0.975; 197} = \pm 1.97$.
  Основная гипотеза отевргается.
  \item Необходимо проверить гипотезу о том, что $\beta_1 = \beta_2$. Иначе говоря,
  что $\beta_1 - \beta_2 = 0$. Найдём стандартную ошибку:
  \[
  \Var(\beta_1 - \beta_2) = 0.125^2 + 0.085^2 - 2 \cdot (-0.0096) = 0.04025 \Rightarrow
  s.e.(\beta_1 - \beta_2) \approx 0.205
  \]
  Тогда $t_{obs} = (0.6029 - 0.375) / 0.205 \approx 1.11$, $t_{0.975; 197} = \pm 1.97$.
  Основная гипотеза не отвергается.
  \item $t_{obs} = -0.964 / 0.707 \approx -1.3635$, $t_{crit} = \pm 1.97$.
  Основная гипотеза не отвергается.
  \item $F_{0.95; 3, 194} = 2.65$
  \[
  F = \frac{(0.954 - 0.9) / 3}{(1 - 0.954) / 194} = 75.913
  \]
  Основная гипотеза отвергается.
\end{enumerate}
\item Выпишем ограниченную и неограниченную модели:
\begin{align*}
  &R: Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + \varepsilon_i \\
  &UR: Y_i = \beta_0 + \Delta_0 d_i + (\beta_1 + \Delta_1 d_i) X_i + (\beta_2 + \Delta_2 d_i) Z_i + \varepsilon_i,
\end{align*}
где $d_i$ дамми-перменная, обозначающая городских жителей:
\[
d_i =
\begin{cases}
1, & \text{ для жителей города} \\
0, & \text{ для сельских жителей}
\end{cases}
\]
Проверять будем следующую гипотезу:
\[
\begin{cases}
H_0: \Delta_0 = \Delta_1 = \Delta_2 = 0 \\
H_a: \Delta_0^2 + \Delta_1^2 + \Delta_2^2 > 0
\end{cases}
\]
Посчитаем значение статистики:
\[
F_{obs} = \frac{(900 - 500) / 3}{500 / (500 - 6)} \approx 131.73
\]
Критическое значение: $F_{0.95; 3, 496} = 2.65$. Основная гипотеза отвергается,
то есть зависимость нельзя считать единой.
\item
\begin{enumerate}
  \item Собственные числа матрицы $\tilde X^T \tilde X$: $\lambda_1 = 1.85$,
  $\lambda_2 = 1$, $\lambda_3 = 0.15$. Параметр обусловленности: $\sqrt{1.85 / 0.15}$.
  \item Выясним, сколько главных компонент нужно найти:
  \begin{align*}
    &\frac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{1.85}{3} = 0.61(6) < 0.7\\
    &\frac{\lambda_1 + \lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{2.85}{3} = 0.95 > 0.7
  \end{align*}
  Необходимо найти две главные компоненты. Например, подойдут следующие:
  \[
  z_1 = \begin{pmatrix}
  1/\sqrt{2} \\ -1 / \sqrt{2} \\ 0
  \end{pmatrix} \qquad
  z_2 = \begin{pmatrix}
  0 \\ 0 \\ 1
  \end{pmatrix}
  \]
\end{enumerate}
\item
\begin{enumerate}
  \item
  \begin{itemize}
    \item Линейная модель. При увеличении общей площади на $1$ м$^2$ продажи
    в расчёте на квадратный метр снизятся на $15.25$ гульденов.
    \item Полулогарифмическая модель. При увеличении общей площади на $1$ м$^2$ продажи
    в расчёте на квадратный метр снизятся на $0.24$\%.
    \item Модель в логарифмах. При увеличении общей площади на $1$\% продажи
    в расчёте на квадратный метр снизятся на $0.31$\%.
  \end{itemize}
  \item Можно воспользоваться PE-тестом. Чтобы выбрать между линейной и полулогарифмической
  моделями, на первом шаге находим оценки $\widehat{\ln Sales}$ и
  $\widehat{Sales}$. На втором — оцениваем вспомогательные регрессии:
  \begin{align*}
    \ln Sales_i &= \gamma_0 + \gamma_1 Size_i + \gamma_2 Nfull_i +
    \gamma_3 Ntemp_i + \gamma_4 Owner_i \\
    &+ \theta_1 (\widehat{Sales}_i - \exp(\widehat{\ln Sales}_i)) +
    \varepsilon_{1i} \\
    Sales_i &= \delta_0 + \delta_1 Size_i + \delta_2 Nfull_i +
    \delta_3 Ntemp_i + \delta_4 Owner_i \\
    &+ \theta_2 (\ln \widehat{Sales}_i -\widehat{\ln Sales}_i) +
    \varepsilon_{2i}
  \end{align*}
  Если коэффициент $\theta_1$ незначим, то выбирается полулогарифимическая модель.
  Если коэффициент $\theta_2$ незначим, то выбирается линейная модель.
  Если оба коэффициента значимы или незначимы одновременно, то выбор сделать невозможно.

  Для выбора между линейной и линейной в логарифмах моделями процедура аналогична.
\end{enumerate}
\end{enumerate}


\subsection{Контрольная работа 3. Базовый поток, 2019-03-30}



\input{tests/2018_2019/kr3/tex_1/exercise01}
\input{tests/2018_2019/kr3/tex_1/exercise02}
\input{tests/2018_2019/kr3/tex_1/exercise03}
\input{tests/2018_2019/kr3/tex_1/exercise04}
\input{tests/2018_2019/kr3/tex_1/exercise05}
\input{tests/2018_2019/kr3/tex_1/exercise06}
\input{tests/2018_2019/kr3/tex_1/exercise07}
\input{tests/2018_2019/kr3/tex_1/exercise08}
\input{tests/2018_2019/kr3/tex_1/exercise09}
\input{tests/2018_2019/kr3/tex_1/exercise10}

\subsubsection*{Задачи}


\begin{enumerate}
  \item Сидоров Вова оценивает два неизвестных параметра: $a$ — где стоят ракеты, $b$ — где продают конфеты.

  Вова оценил параметры методом максимального правдоподобия и получил оценки $\hat a = 1.5$, $\hat b = 2.5$.
  Затем Вова решил проверить гипотезу $H_0$: $a=1$ и $b=2$.

  Значения функции правдоподобия, градиента и оценённой информации Фишера в двух точках
  частично приведены в таблице:


\begin{tabular}{lccc}
\toprule
Точка & $\ell(a, b)$ & $(\ell'_a, \ell'_b)$ &  $\hat I_F$ \\
\midrule
$a=1.5$, $b=2.5$ & -200 &  ? &
$\begin{pmatrix}
16 & -1 \\
-1 & 20 \\
\end{pmatrix}$ \\
$a=1$, $b=2$ & -250 &  $(2, -1)$ &
$\begin{pmatrix}
10 & -1 \\
-1 & 15 \\
\end{pmatrix}$ \\
\bottomrule
\end{tabular}

Помогите Сидорову Вове!

\begin{enumerate}
  \item Заполните пропуск в таблице;
  \item Проверьте гипотезу $H_0$ тремя способами: с помощью $LR$, $LM$ и $W$ статистик.
\end{enumerate}

\item По 200 наблюдениям исследователь Иннокентий оценил модель логистической регрессии для вероятности
сдать экзамен по метрике:
\[
\hat \P(Y_i = 1) = \Lambda(1.5 + 0.3X_i - 0.4 D_i),
\]
где $Y_i$ — бинарная переменная равная 1, если студент сдал экзамен;
$X_i$ — количество часов подготовки студента; $D_i$ — бинарная переменная равная 1,
если студент пробовал пиццу «четыре сыра» в новой столовой.

Оценка ковариационной матрицы оценок коэффициентов имеет вид:

\[
\begin{pmatrix}
0.04 & -0.01 & 0 \\
-0.01 & 0.01 & 0 \\
0 & 0 & 0.09 \\
\end{pmatrix}
\]

\begin{enumerate}
  \item Проверьте гипотезу о том, что количество часов подготовки не влияет на вероятность сдать экзамен.
  \item Посчитайте предельный эффект увеличения каждого регрессора на вероятность сдать экзамен для студента не пробовавшего пиццу и готовившегося 24 часа.
Кратко, одной-двумя фразами, прокомментируйте смысл полученных цифр.
%  \item Постройте 95\%-й доверительный интервал для разницы вероятностей сдать экзамен двумя студентами, если
%  оба студента готовились 20 часов, однако один пробовал пиццу, а второй — нет.
  \item При каком значении $D_i$ предельный эффект увеличения $X_i$ на вероятность сдать экзамен максимален,
  если $X_i=20$?
\end{enumerate}



\newpage
\item Билл Гейтс оценил регрессию $\hat Y_i = 4 + 0.4 X_i + 0.9 W_i$, $RSS = 520$, $R^2 = 2/15$.

Про матрицу регрессоров $X$ известно, что
\[
X'X = \begin{pmatrix}
	29 & 0 & 0 \\
	0 & 50 & 10 \\
	0 & 10 & 80 \\
\end{pmatrix}
\]

\begin{enumerate}
 \item Сколько наблюдений было у Билла Гейтса?
 \item Найдите выборочное среднее переменных $X$, $W$ и $Y$.
 \item Постройте 95\%-й доверительный интервал для значения зависимой (индивидуальный прогноз) переменной при $X=1$ и $W=3$.
\end{enumerate}

\item Величины $X_1$, \ldots, $X_{100}$ распределены независимо и равномерно на отрезке $[-3a;5a]$.
	Оказалось, что $\sum_{i=1}^{100} X_i = 200$ и $\sum_{i=1}^{100}|X_i| = 500$.

\begin{enumerate}
  \item Оцените параметр $a$ методом моментов, используя момент $\E(X_i)$.
  \item Оцените параметр $a$ обобщённым методом моментов, используя моменты $\E(X_i)$ и $\E(|X_i|)$, и взвешивающую матрицу $W = \begin{pmatrix}
		  3 & 0 \\
		  0 & 64 \\
	  \end{pmatrix}$.
\end{enumerate}

\item Контора «Рога и Копыта» определяет
	необходимый запас рогов, $Y$, в зависимости от ожидаемых годовых продаж рогов,
	$X^e$, по формуле $Y_i = \beta_0 + \beta_1 X^e_i$. Коэффициенты $\beta_0$ и $\beta_1$ держатся в строжайшей тайне!

	В распоряжении холдинга «Рог изобилия» оказались данные
	по запасам рогов, $Y$, и фактическим годовым продажам рогов, $X$, конторы «Рога и Копыта». Фактические продажи рогов связаны с ожидаемыми уравнением $X_i = X_i^e + u_i$.

Исследователи холдинга хотят оценить секретные коэффициенты $\beta_0$ и $\beta_1$ с помощью простой регрессии $\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i$ методом наименьших квадратов.

\begin{enumerate}

	\item Найдите предел по вероятности для $\hat \beta_1$ и $\hat\beta_0$. Являются ли оценки состоятельными?
	\item Если оценки не являются состоятельными, то по шагам опишите алгоритм получения состоятельных оценок. Если алгоритм требует получения дополнительных переменных, то укажите, какими свойствами они должны обладать.

\end{enumerate}


Векторы $(X_i^e, u_i)$ одинаково распределены при любом $i$ и независимы.



\end{enumerate}


\subsection{Контрольная работа 3. ИП, 2019-03-30}




Ровно 228 лет назад, 30 марта 1791 Национальное собрание Франции ввело определение метра: одна сорокамиллионная часть длины парижского меридиана.


\begin{enumerate}
\item Рассмотрим логит-модель $\P(y_i = 1) = \Lambda(X_{i.}\beta)$, где $X_{i.}$ — $i$-ая строчка матрицы
регрессоров. Людовик XIV знает, что оценки логит-модели в явном виде аналитически не считаются. Поэтому он использует две технологии.

Технология 1. Разложить лог-функцию правдоподобия в ряд Тейлора до членов второго порядка в окрестности $\beta=0$ и максимизировать полученную функцию.

Технология 2. Стартуя из точки $\beta = 0$ сделать один шаг равный градиенту лог-функции правдоподобия.


\begin{enumerate}
  \item Помогите Людовику получить обе аппроксимации оценок логит-модели.
  \item С каким известным алгоритмом совпадает одна из этих оценок?
\end{enumerate}


\item Храбрый исследователь Шарль Ожье́ де Бац де Кастельмо́р, граф д'Артанья́н, оценивает модель $y_i = \beta x_i + u_i$ по трём наблюдениям.
Ошибки $u_i$ имеют многомерное нормальное распределение с нулевым ожиданием и ковариационной матрицей
$\sigma^2\begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 1 & 2 \\
\end{pmatrix}$.

Наблюдения известны:

\begin{tabular}{cccc}
\toprule
$y_i$ & 1 & 2 & 1  \\
$x_i$ & 0 & 3 & -1 \\
\bottomrule
\end{tabular}


\begin{enumerate}
  \item Найдите наиболее эффективную оценку в классе линейных по игреку несмещённых оценок.
  \item Какой уровень доверия обеспечивает интервал $[\hb -1;\hb +1]$?
  \item Постройте 95\%-й предиктивный интервал для $y_4$, если известно, что $x_4=1$,
	а $u_4$ не зависит от предыдущих ошибок и имеет дисперсию $3\sigma^2$.
\end{enumerate}


\item Монетку подбросили 100 раз. Мария Антуанетта помнит, что в первых 70-и бросках было 30 орлов.
А графиня де Полиньяк помнит, что в последних 60-и бросках было 20 орлов.


\begin{enumerate}
  \item Постройте GMM оценку вероятности орла с единичной взвешивающей матрицей.
  \item Постройте оптимальную GMM оценку.
  \item Постройте 95\% доверительный интервал, используя каждую из оценок
\end{enumerate}

\newpage
\item Подарок для тех, кто прорешал прошлогодний вариант :)


   Исследовательница Несмеяна вывела хитрую формулу для $\hat a$ — произвольной несмещённой оценки неизвестного векторного параметра $a$. Оценка $\hat a$ не обязана быть оценкой метода максимального правдподобия!
    Обозначим $s(a)$ — вектор-столбец градиент логарифмической функции правдоподобия. Докажите, что для оценки Несмеяны выполнено неравенство Крамера-Рао,
    а именно, что матрица $M$  положительна определена.

\[
M = \Var(s(a))\cdot \Var(\hat a) - I_{k\times k}.
\]


Подсказки:

    \begin{enumerate}
      \item Вспомните, чему равно $\E(s(a))$. Достаточно просто вспомнить, доказывать не требуется!
      \item Найдите скаляры $\Cov\left(\hat a_1, \frac{\partial \ell}{\partial a_1}\right)$,
	$\Cov\left(\hat a_1, \frac{\partial \ell}{\partial a_2}\right)$
	и матрицу $\Cov\left(\hat a, s(a) \right)$.
      \item Рассмотрим два произвольных случайных вектора $R$ и $S$ и два вектора констант подходящей длины $\alpha$ и $\beta$.
	Найдите минимум функции $f(\alpha, \beta) = \Var(\alpha^T R + \beta^T S)$ по $\beta$.
	Выпишите явно $\beta^*(\alpha)$ и $f^*(\alpha)$.
      \item Докажите, что для произвольных случайных векторов положительно определена матрица
	\[
          \Var(R) - \Cov(R, S) \Var^{-1}(S)\Cov(S, R)
	\]
      \item Завершите доказательство векторного неравенства Крамера-Рао.


    \end{enumerate}

  Без угрызений совести можно храбро переставлять интегралы и производные :)


  \item
  Докажите, что в методе главных компонент с масштабированием переменных средняя величина $R^2$ по всем парным
  регрессиям исходных переменных на первую главную компоненту равна наибольшему сингулярному значению
  матрицы исходных переменных.

\item Докажите закон больших чисел в форме Бернштайна.

Если величины $y_1$, $y_2$, \ldots, $y_n$ имеют одинаковое ожидание, $\E(y_i) = \mu$, ограниченную дисперсию и $\Cov(y_i, y_j) \to 0$ при $|i - j|\to \infty$, то $\bar y$ сходится по вероятности к $\mu$.

\end{enumerate}

Заметки: писали около 2.5 часов, без чит-листа, но с правом посмотреть 5 минут любой печатный источник в течение экзамена.

\subsection{Финальный экзамен}


\subsubsection*{Тест}
\input{tests/2018_2019/final/tex_01/exercise01}
\input{tests/2018_2019/final/tex_01/exercise02}
\input{tests/2018_2019/final/tex_01/exercise03}
\input{tests/2018_2019/final/tex_01/exercise04}
\input{tests/2018_2019/final/tex_01/exercise05}
\input{tests/2018_2019/final/tex_01/exercise06}
\input{tests/2018_2019/final/tex_01/exercise07}
\input{tests/2018_2019/final/tex_01/exercise08}
\input{tests/2018_2019/final/tex_01/exercise09}
\input{tests/2018_2019/final/tex_01/exercise10}


\subsubsection*{Задачи}

\begin{enumerate}
\item Рассмотрим систему одновременных уравнений

\[
\begin{cases}
	Y_1 = \gamma_1 Y_2 + \beta_{11} X_{1} + \beta_{12} X_2 + \beta_{13} X_3  + \varepsilon_1 \\
	Y_2 = \gamma_2 Y_1 + \beta_{21} X_{1} + \beta_{22} X_2 + \beta_{23} X_3  + \varepsilon_2 \\
\end{cases}.
\]

Здесь $Y_j$ — эндогенные переменные, а $X_j$ — экзогенные. 
С помощью условий ранга и порядка проверьте идентифицируемость системы

\begin{enumerate}
	\item в общем случае;
	\item при наложении дополнительных ограничений $\gamma_1 - \gamma_2 = \beta_{23} = 0$.
\end{enumerate}

\item Исследователь оценил функцию издержек для шести фирм по четырём годам двумя способами.
С помощью сквозной регрессии 
$\widehat{\ln C_{it}} = -4.2 + 0.89 \ln Q_{it}, \; R^2 = 0.97$.
И включив дополнительно в модель дамми-переменные:
\[
\widehat{\ln C_{it}} =  -2.6D_{1i} -2.9D_{2i} - 2.4D_{3i} -2.1D_{4i}-2.3D_{5i}-1.9D_{6i} +  0.66 \ln Q_{it}, \; R^2 = 0.99.
\]
Здесь $Q_{it}$ — выпуск фирмы $i$ в год $t$, $C_{it}$ — издержки фирмы $i$ в год $t$, 
а дамми-переменные $D_{ji}$ равны единице при $i=j$ и нулю иначе.

С помощью подходящего теста выберите наилучшую из двух рассмотренных моделей на уровне значимости 5\%.

\item Рассмотрим модель зависимости потребления от дохода для Канады 1926-1949 годов. 
\[
\begin{cases}
	C_t^* = b_1 + b_2 W_t + b_3 NW_t + b_4 A_t + u_t \\
	C_t - C_{t-1} = \lambda (C_t^* - C_{t-1})
\end{cases},
\]
где $C_t$ — совокупное потребление, $W_t$ — совокупная заработная плата, $NW_t$ — совокупный доход за вычетом заработной платы,
$A_t$ — дамми-переменная, равная единице для довоенного периода и нулю — иначе. 
Потребление и доход измерены в миллиардах канадских долларов. 

\begin{enumerate}
	\item Приведите исходную модель к виду $C_t = a_1 + a_2 W_t + a_3 NW_t + a_4 C_{t-1} + a_5 A_t + \varepsilon_t$.
	\item Как можно получить состоятельные оценки коэффициентов в преобразованном уравнении?
	\item Оценённое уравнение имеет вид:
	$\hat C_t = \underset{(1.2)}{0.9} + \underset{(0.25)}{0.61}W_t + \underset{(0.09)}{0.28}NW_t + \underset{(0.04)}{0.22}C_{t-1} + \underset{(0.13)}{0.69}A_t$.
	Найдите краткосрочную и долгосрочную предельную склонность к потреблению по зарплате. 
	\item Как проверить, есть ли в преобразованной модели автокорреляция остатков?
\end{enumerate}

\item Рассмотрим стационарный процесс, удовлетворяющий уравнению $Y_t = 1 + 0.6Y_{t-1} + u_t - 0.3 u_{t-1}$, 
где $u_t$ — белый шум с $u_t \sim \cN(0; 4)$.

\begin{enumerate}
	\item Найдите $\E(Y_t)$, $\Var(Y_t)$.
	\item Найдите первые два значения автокорреляционной и частной автокорреляционной функций.
	\item Постройте 95\% интервальный прогноз для $Y_{101}$, если известно, что $Y_{100} = 2$, $u_{100}=1$.
\end{enumerate}

\end{enumerate}

\subsection{Финальный экзамен. Краткие ответы}

Тест: CAAAD AECBC

\begin{enumerate}
  \item Без дополнительных ограничений уже по критерию порядка система не идентифицируема. 
  Поэтому без ограничений критерий ранга можно даже не проверять.
  При наложении ограничений первое уравнение остаётся неидентифицируемым, второе — становится идентифицируемым.
  \item   
  Гипотеза $H_0$: $\gamma_1 = \gamma_2 = \gamma_3 = \gamma_4 = \gamma_5 = \gamma_6$, $H_a$ — хотя бы одно равенство нарушено.
  
  \[
  F = \frac{(0.03 - 0.01) / 5}{0.01 / (24-7)}  
  \]

  \item 
  \begin{enumerate}
    \item $C_t = \lambda b_1 + \lambda b_2 W_t + \lambda b_3 NW_t + (1-\lambda) C_{t-1} + \lambda b_4 A_t + \lambda u_t$
    \item Ошибка некоррелирована с регрессорами, поэтому для состоятельных оценок можно применять обычный МНК.
    \item Краткосрочный эффект $0.61$, долгосрочный — $0.61/(1-0.22)$.
    \item Подойдёт тест Бройша-Годфри. Заметим, что тест Дарбина-Уотсона не применим, так как требует строгой 
    экзогенности регрессоров, а здесь среди регрессоров есть $C_{t-1}$.
  \end{enumerate}

  \item 
  \begin{enumerate}
    \item $\E(Y_t) = 1 + 0.6 \E(Y_{t-1})$, следовательно, $\E(Y_t) = 2.5$.
    \item Расписываем $Y_t$ через $u_t$, $u_{t-1}$ и более ранние ошибки, получаем
    \[
    Y_t = 1 + 0.6 \cdot 1 + u_t + 0.3 u_{t-1} + \ldots  
    \]
    Отсюда замечаем, что $\Cov(Y_t, u_t) = 4$, $\Cov(Y_t, u_{t-1}) = 0.3\cdot 4 = 1.2$.
    
    Находим систему на ковариации:
    \[
    \begin{cases}
      \gamma_0 = \Cov(Y_t, Y_t) = \Cov(Y_t, 1 + 0.6Y_{t-1} + u_t - 0.3 u_{t-1}) = 0.6\gamma_1 + 4 - 0.3\cdot 1.2 \\
      \gamma_1 = \Cov(Y_{t-1}, Y_t) = \Cov(Y_{t-1}, 1 + 0.6Y_{t-1} + u_t - 0.3 u_{t-1}) = 0.6\gamma_0 - 0.3 \cdot 4 \\
      \gamma_2 = \Cov(Y_{t-2}, Y_t) = \Cov(Y_{t-2}, 1 + 0.6Y_{t-1} + u_t - 0.3 u_{t-1}) = 0.6\gamma_1 \\
    \end{cases}
    \]
    Корреляции находим по правилу $\rho_k = \gamma_k / \gamma_0$.

    Решая систему, находим, что $\gamma_0 = 4.56$, $\rho_1 = 0.34$, $\rho_2 = 0.20$

    \item Частную корреляцию находим из обычной с помощью соотношений, $\phi_{11} = \rho_1 = 0.34$, 
    $\phi_{22} = \frac{\rho_2 - \rho_1^2}{1-\rho_1^2} = 0.10$.
    \item Для предиктивного интервала находим условные ожидание и дисперсию: 
    \[
    \E(Y_{100} \mid \mathcal{F}_{100}) = 1 + 0.6 \cdot 2 - 0.2 \cdot 1 = 1.9
    \]
    \[
    \Var(Y_{100} \mid \mathcal{F}_{100}) = \sigma^2_{u} = 4
    \]
    Поэтому интервал имеет вид:
    \[
    [1.9 - 1.96 \sqrt{4} ; 1.9 + 1.96 \sqrt{4}]  
    \]
  \end{enumerate}
  
  
\end{enumerate}
